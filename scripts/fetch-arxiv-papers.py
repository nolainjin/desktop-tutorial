#!/usr/bin/env python3
"""
arXiv APIë¥¼ ì‚¬ìš©í•´ì„œ ë…¼ë¬¸ ë°ì´í„° ìˆ˜ì§‘
ê° ë¶„ì•¼ë³„ë¡œ 2010ë…„ ì´í›„ ì¸ìš©ì´ ë§ì€ ë…¼ë¬¸ 100ê°œì”© ìˆ˜ì§‘
"""

import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
import json
import time
from datetime import datetime

# arXiv ì¹´í…Œê³ ë¦¬ ë§¤í•‘
ARXIV_CATEGORIES = {
    'ìˆ˜í•™': ['math.AG', 'math.AT', 'math.CO', 'math.NT', 'math.DG'],
    'ë¬¼ë¦¬í•™': ['physics.gen-ph', 'astro-ph', 'cond-mat', 'quant-ph', 'hep-th'],
    'ì»´í“¨í„°ê³¼í•™': ['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL', 'cs.RO'],
}

def fetch_arxiv_papers(category, max_results=100, start_year=2010):
    """arXivì—ì„œ ë…¼ë¬¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    base_url = 'http://export.arxiv.org/api/query?'

    # ê²€ìƒ‰ ì¿¼ë¦¬: ì¹´í…Œê³ ë¦¬ë³„ ìµœì‹  ë…¼ë¬¸
    search_query = f'cat:{category}'

    params = {
        'search_query': search_query,
        'start': 0,
        'max_results': max_results,
        'sortBy': 'submittedDate',
        'sortOrder': 'descending'
    }

    url = base_url + urllib.parse.urlencode(params)

    try:
        print(f"Fetching papers from category: {category}")
        # User-Agent ì¶”ê°€
        req = urllib.request.Request(url, headers={
            'User-Agent': 'Mozilla/5.0 (Academic Research Bot) Python/3.x'
        })
        with urllib.request.urlopen(req) as response:
            data = response.read()

        # XML íŒŒì‹±
        root = ET.fromstring(data)
        namespace = {'atom': 'http://www.w3.org/2005/Atom',
                    'arxiv': 'http://arxiv.org/schemas/atom'}

        papers = []
        for entry in root.findall('atom:entry', namespace):
            # ì œëª©
            title_elem = entry.find('atom:title', namespace)
            title = title_elem.text.strip().replace('\n', ' ') if title_elem is not None else ''

            # ì €ìë“¤
            authors = []
            for author in entry.findall('atom:author', namespace):
                name_elem = author.find('atom:name', namespace)
                if name_elem is not None:
                    authors.append(name_elem.text)

            # ì¶œíŒì¼
            published_elem = entry.find('atom:published', namespace)
            published = published_elem.text if published_elem is not None else ''
            year = int(published[:4]) if published else 0

            # 2010ë…„ ì´í›„ë§Œ
            if year < start_year:
                continue

            # arXiv ID ë° ë§í¬
            id_elem = entry.find('atom:id', namespace)
            arxiv_id = id_elem.text if id_elem is not None else ''

            # ìš”ì•½ (í‚¤ì›Œë“œ ìƒì„±ìš©)
            summary_elem = entry.find('atom:summary', namespace)
            summary = summary_elem.text.strip() if summary_elem is not None else ''

            # ì¹´í…Œê³ ë¦¬
            category_elem = entry.find('arxiv:primary_category', namespace)
            primary_cat = category_elem.get('term') if category_elem is not None else category

            papers.append({
                'title': title,
                'authors': authors[:3],  # ìµœëŒ€ 3ëª…
                'year': year,
                'url': arxiv_id,
                'category': primary_cat,
                'summary': summary[:200]  # ì²˜ìŒ 200ìë§Œ
            })

        print(f"  Found {len(papers)} papers")
        return papers

    except Exception as e:
        print(f"  Error fetching {category}: {e}")
        return []

def collect_papers_by_field(field_name, categories, target_count=100):
    """íŠ¹ì • ë¶„ì•¼ì˜ ë…¼ë¬¸ ìˆ˜ì§‘"""
    all_papers = []
    per_category = target_count // len(categories)

    print(f"\n=== Collecting {field_name} papers ===")

    for cat in categories:
        papers = fetch_arxiv_papers(cat, max_results=per_category)
        all_papers.extend(papers)
        time.sleep(3)  # API ì œí•œ ëŒ€ì‘

    # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
    seen_titles = set()
    unique_papers = []
    for paper in all_papers:
        if paper['title'] not in seen_titles:
            seen_titles.add(paper['title'])
            unique_papers.append(paper)

    return unique_papers[:target_count]

def generate_keywords_from_summary(summary):
    """ìš”ì•½ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)"""
    # ê°„ë‹¨íˆ ì²˜ìŒ ëª‡ ë‹¨ì–´ë¥¼ í‚¤ì›Œë“œë¡œ
    words = summary.split()[:5]
    return [w.strip('.,;:()[]{}') for w in words if len(w) > 3]

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    all_results = {}

    # arXiv ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘
    for field_name, categories in ARXIV_CATEGORIES.items():
        papers = collect_papers_by_field(field_name, categories, target_count=100)

        # í‚¤ì›Œë“œ ìƒì„±
        for paper in papers:
            paper['keywords'] = generate_keywords_from_summary(paper.get('summary', ''))
            del paper['summary']  # ìš”ì•½ ì œê±°
            del paper['category']  # ì¹´í…Œê³ ë¦¬ ì œê±°

        all_results[field_name] = papers
        print(f"  Collected {len(papers)} papers for {field_name}")

    # JSONìœ¼ë¡œ ì €ì¥
    output_file = 'arxiv_papers_collected.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… Saved to {output_file}")

    # í†µê³„ ì¶œë ¥
    print("\nğŸ“Š Statistics:")
    for field, papers in all_results.items():
        print(f"  - {field}: {len(papers)}ê°œ")
    print(f"  Total: {sum(len(p) for p in all_results.values())}ê°œ")

if __name__ == '__main__':
    main()
