# IdeaConnect v2.0 ì™„ë²½ ê°€ì´ë“œ
## ì´ë¡ ì  ê¹Šì´ + ì‹¤ìš©ì  êµ¬í˜„ + UX í˜ì‹ 

**ë²„ì „**: 2.0
**ì‘ì„±ì¼**: 2025-11-10
**ê¸°ì¤€**: ì–¸ì–´í•™Â·ì¸ì§€ì‹¬ë¦¬í•™Â·AI í•™ì œê°„ í†µí•© + ì‹¤ì „ êµ¬í˜„

---

# Part 1: ë¹„ì „ & ì´ë¡ ì  ê¸°ë°˜

## 1.1 ëª©í‘œ ì¬ì •ì˜

### ê¸°ì¡´ ëª©í‘œì˜ ë¬¸ì œì 

âŒ **ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­ ì¤‘ì‹¬** â†’ ë§¥ë½ ì†ì‹¤
- "ìŠµê´€"ì´ë¼ëŠ” ë‹¨ì–´ë§Œ ì°¾ì•„ì„œ ì—°ê²°
- ê¹Šì€ ì˜ë¯¸ ì—°ê²° ë¶€ì¬
- ì‚¬ìš©ìê°€ ê¸°ëŒ€í•˜ì§€ ëª»í•œ ë°œê²¬ ì–´ë ¤ì›€

âŒ **ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì¤‘ì‹¬** â†’ ê°œë… ê°„ ê´€ê³„ ë¶€ì¬
- "ëª…ì–¸", "ì±…", "ì˜í™”"ë¡œë§Œ ë¶„ë¥˜
- ì•„ì´ë””ì–´ ê°„ ìœ ê¸°ì  ì—°ê²° ë¯¸í¡
- ì§€ì‹ì˜ ë„¤íŠ¸ì›Œí¬ê°€ ì•„ë‹Œ ë‹¨ìˆœ ë¦¬ìŠ¤íŠ¸

âŒ **ê²€ìƒ‰ ìµœì í™” ì¤‘ì‹¬** â†’ ë°œê²¬(discovery) ê²½í—˜ ë¶€ì¡±
- ê²€ìƒ‰í•˜ë©´ ë‚˜ì˜¤ëŠ” ê²ƒë§Œ ë³¼ ìˆ˜ ìˆìŒ
- ìš°ì—°í•œ ë°œê²¬(ì„¸ë Œë””í”¼í‹°) ë¶€ì¬
- ì°½ì˜ì  ì—°ê²° ê²½í—˜ ì œí•œ

### ìƒˆë¡œìš´ ë¹„ì „

> **"ì‚¬ê³ ì˜ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•˜ì—¬ ì•„ì´ë””ì–´ ê°„ ìœ ê¸°ì  ì—°ê²°ê³¼ ì°½ë°œì  ë°œê²¬ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤"**

#### ì •ëŸ‰ì  ëª©í‘œ

| ì§€í‘œ | ëª©í‘œ | ì„¤ëª… |
|------|------|------|
| **ë…¸ë“œ(ì•„ì´ë””ì–´)** | 50,000ê°œ ì´ìƒ | ë‹¤ì–‘í•œ ì¶œì²˜ì˜ ê³ í’ˆì§ˆ ì¸ì‚¬ì´íŠ¸ |
| **ì—£ì§€(ê´€ê³„)** | 500,000ê°œ ì´ìƒ | í‰ê·  ë…¸ë“œë‹¹ 10ê°œ ì—°ê²° |
| **ê´€ê³„ë§ êµ¬ì¡°** | ë‹¤ì¸µì  | ì˜ë¯¸ì , ê°ì •ì , ì‹¤ìš©ì , ì—­ì‚¬ì  |
| **ê²€ìƒ‰ ì •í™•ë„** | 85% ì´ìƒ | ìƒìœ„ 5ê°œ ê²°ê³¼ ì¤‘ 4ê°œ ì´ìƒ ê´€ë ¨ì„± ë†’ìŒ |

#### ì •ì„±ì  ëª©í‘œ

1. **ë§¥ë½ì  ìœ ì‚¬ë„**: í‘œë©´ì  í‚¤ì›Œë“œê°€ ì•„ë‹Œ ì‹¬ì¸µ ì˜ë¯¸ ì—°ê²°
2. **ì°½ë°œì  ë°œê²¬**: ì˜ˆìƒì¹˜ ëª»í•œ ì•„ì´ë””ì–´ ê°„ ì—°ê²° ì§€ì›
3. **ì¸ì§€ ë¶€í•˜ ìµœì í™”**: ì •ë³´ ê³¼ë¶€í•˜ ì—†ì´ í†µì°° ì œê³µ
4. **ê°œì¸í™” ê°€ëŠ¥ì„±**: ì‚¬ìš©ì ì‚¬ê³  íŒ¨í„´ í•™ìŠµ ê¸°ë°˜

---

## 1.2 ì¸ì§€ì‹¬ë¦¬í•™ì  ì„¤ê³„

### ì›ë¦¬ 1: ìŠ¤í‚¤ë§ˆ ì´ë¡  (Schema Theory)

**í•µì‹¬ ê°œë…**: ì¸ê°„ì€ ê°œë…ì„ ë…ë¦½ëœ ì ì´ ì•„ë‹Œ, ì—°ê²°ëœ êµ¬ì¡°(ìŠ¤í‚¤ë§ˆ)ë¡œ ì €ì¥í•œë‹¤.

**ì°½ì‹œì**: Jean Piaget, Frederic Bartlett

**IdeaConnect ì ìš©**:
```typescript
interface SchemaMapping {
  schema_type: string;        // "ìŠµê´€ í˜•ì„± ìŠ¤í‚¤ë§ˆ"
  slot: string;               // "ê²°ê³¼"
  related_slots: {
    ì›ì¸: string[];           // ["ë°˜ë³µ", "ì‹¤ì²œ"]
    ë©”ì»¤ë‹ˆì¦˜: string[];       // ["ì‹ ê²½ê°€ì†Œì„±", "ìë™í™”"]
    ì‹œê°„ì„±: string[];         // ["ì¥ê¸°ì ", "ëˆ„ì ì "]
  };
}

// ì˜ˆì‹œ ë°ì´í„°
{
  id: "node_001",
  content: "We are what we repeatedly do. Excellence, then, is not an act, but a habit.",
  schema_mappings: [
    {
      schema_type: "ìŠµê´€ í˜•ì„± ìŠ¤í‚¤ë§ˆ",
      slot: "ê²°ê³¼",
      related_slots: {
        ì›ì¸: ["ë°˜ë³µ", "ì‹¤ì²œ"],
        ë©”ì»¤ë‹ˆì¦˜: ["ì‹ ê²½ê°€ì†Œì„±", "ìë™í™”"],
        ì‹œê°„ì„±: ["ì¥ê¸°ì ", "ëˆ„ì ì "]
      }
    }
  ]
}
```

**ì¹´í…Œê³ ë¦¬ â†’ ìŠ¤í‚¤ë§ˆ ë³€í™˜í‘œ**:

| ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ | ìŠ¤í‚¤ë§ˆ | í•˜ìœ„ ìŠ¤í‚¤ë§ˆ |
|-------------|--------|------------|
| ìŠµê´€ | í–‰ë™ ë³€í™” ìŠ¤í‚¤ë§ˆ | ìŠµê´€ í˜•ì„±, ìŠµê´€ ê¹¨ê¸°, ìë™í™” |
| ì„±ì¥ | ë°œë‹¬ ìŠ¤í‚¤ë§ˆ | í•™ìŠµ, ì ì‘, ì§„í™”, ê·¹ë³µ |
| ê´€ê³„ | ì‚¬íšŒì  ì¸ì§€ ìŠ¤í‚¤ë§ˆ | ê³µê°, ì†Œí†µ, ê°ˆë“±, ì‹ ë¢° |
| ì‹œê°„ | ì‹œê°„ ì¸ì‹ ìŠ¤í‚¤ë§ˆ | í˜„ì¬ ì¤‘ì‹¬, ë¯¸ë˜ ì§€í–¥, ê³¼ê±° ì„±ì°° |
| ëª©í‘œ | ë™ê¸° ìŠ¤í‚¤ë§ˆ | ë‚´ì¬ ë™ê¸°, ì™¸ì¬ ë™ê¸°, ìê¸°íš¨ëŠ¥ê° |

**êµ¬í˜„ íš¨ê³¼**:
- ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­ì„ ë„˜ì–´ì„  ê°œë…ì  ì—°ê²°
- ì‚¬ìš©ìê°€ "ìŠµê´€"ì„ ê²€ìƒ‰í•˜ë©´ "ì •ì²´ì„±", "ë°˜ë³µ", "ì‹œê°„" ë“± ê´€ë ¨ ìŠ¤í‚¤ë§ˆ ë…¸ë“œ ìë™ ì—°ê²°
- ë” í’ë¶€í•œ íƒìƒ‰ ê²½í—˜

---

### ì›ë¦¬ 2: í™•ì‚° í™œì„±í™” ì´ë¡  (Spreading Activation)

**í•µì‹¬ ê°œë…**: í•˜ë‚˜ì˜ ê°œë…ì´ í™œì„±í™”ë˜ë©´ ê´€ë ¨ëœ ê°œë…ë“¤ì´ ì—°ì‡„ì ìœ¼ë¡œ í™œì„±í™”ëœë‹¤.

**ì°½ì‹œì**: Allan Collins, Elizabeth Loftus (1975)

**IdeaConnect ì ìš©**:
```javascript
// ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê´€ê³„ë§
{
  from: "ìŠµê´€",
  to: "ì •ì²´ì„±",
  edge_type: "ì¸ê³¼ê´€ê³„",
  weight: 0.87,              // ê°•í•œ ì—°ê²°
  activation_decay: 0.15,    // ì „íŒŒ ê°ì‡ ìœ¨
  bidirectional: true
}
```

**ê´€ê³„ ìœ í˜•ë³„ ê°€ì¤‘ì¹˜**:

| ê´€ê³„ ìœ í˜• | ê°€ì¤‘ì¹˜ ë²”ìœ„ | ê°•ë„ | ì˜ˆì‹œ |
|----------|------------|------|------|
| **ì¸ê³¼ê´€ê³„** (A â†’ B) | 0.8-0.95 | ê°•í•¨ | "ë°˜ë³µ" â†’ "ìŠµê´€" |
| **ìœ ì‚¬ê´€ê³„** (A â‰ˆ B) | 0.6-0.8 | ì¤‘ê°„ | "ê¾¸ì¤€í•¨" â‰ˆ "ì¸ë‚´" |
| **ëŒ€ì¡°ê´€ê³„** (A â‰  B) | 0.5-0.7 | ì•½í•¨ | "ê³„íš" â‰  "ì¦‰í¥" |
| **ë§¥ë½ì ** | 0.4-0.6 | ì•½í•¨ | "ì•„ì¹¨" contextualâ†’ "ë£¨í‹´" |

**ì „íŒŒ ì•Œê³ ë¦¬ì¦˜**:
```python
def spreading_activation(start_node, max_depth=3, threshold=0.5):
    """
    ì‹œì‘ ë…¸ë“œë¡œë¶€í„° í™œì„±í™”ë¥¼ ì „íŒŒ
    """
    activation = {start_node: 1.0}
    queue = [(start_node, 1.0, 0)]  # (node, activation, depth)

    while queue:
        node, act, depth = queue.pop(0)

        if depth >= max_depth:
            continue

        # ì—°ê²°ëœ ë…¸ë“œë“¤ì—ê²Œ í™œì„±í™” ì „íŒŒ
        for edge in get_edges(node):
            new_activation = act * edge.weight * (1 - edge.activation_decay)

            if new_activation >= threshold:
                neighbor = edge.to

                # ê¸°ì¡´ í™œì„±í™”ì™€ ë¹„êµí•˜ì—¬ ë” ë†’ìœ¼ë©´ ì—…ë°ì´íŠ¸
                if neighbor not in activation or activation[neighbor] < new_activation:
                    activation[neighbor] = new_activation
                    queue.append((neighbor, new_activation, depth + 1))

    return activation
```

**ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ**:
```
ì‚¬ìš©ìê°€ "ìŠµê´€" ë©”ëª¨ ì‘ì„±
  â†“ (weight: 0.87)
"ì •ì²´ì„±" ì•„ì´ë””ì–´ í™œì„±í™”
  â†“ (weight: 0.75)
"ê°€ì¹˜ê´€" ì•„ì´ë””ì–´ í™œì„±í™”
  â†“ (weight: 0.68)
"ì„ íƒ" ì•„ì´ë””ì–´ í™œì„±í™”

â†’ ì‚¬ìš©ìëŠ” "ìŠµê´€ â†’ ì •ì²´ì„± â†’ ê°€ì¹˜ê´€ â†’ ì„ íƒ"ì˜ ì—°ê²° ê³ ë¦¬ ë°œê²¬!
```

---

### ì›ë¦¬ 3: ì •êµí™” ê°€ëŠ¥ì„± ëª¨ë¸ (Elaboration Likelihood Model)

**í•µì‹¬ ê°œë…**: ì •ë³´ ì²˜ë¦¬ ê¹Šì´ì— ë”°ë¼ ì¤‘ì‹¬ ê²½ë¡œ(ê¹Šì€ ì‚¬ê³ )ì™€ ì£¼ë³€ ê²½ë¡œ(íœ´ë¦¬ìŠ¤í‹±) êµ¬ë¶„

**ì°½ì‹œì**: Richard Petty, John Cacioppo (1986)

**IdeaConnect ì ìš©**: ì´ì¤‘ ê²€ìƒ‰ ê²½ë¡œ

```typescript
interface SearchResult {
  // ì¤‘ì‹¬ ê²½ë¡œ: ê¹Šì€ ì˜ë¯¸ ë§¤ì¹­ (ì‚¬ìš©ìê°€ ì§„ì§€í•˜ê²Œ íƒìƒ‰í•  ë•Œ)
  deep_matches: {
    content: string;
    semantic_similarity: number;  // 0.8+
    reasoning: string;             // ì™œ ì—°ê²°ë˜ëŠ”ì§€ ì„¤ëª…
    evidence: string[];            // ì—°ê²° ê·¼ê±°
  }[];

  // ì£¼ë³€ ê²½ë¡œ: ë¹ ë¥¸ ì—°ìƒ (ë¸Œë¼ìš°ì§• ëª¨ë“œ)
  quick_associations: {
    content: string;
    association_type: 'keyword' | 'emotion' | 'metaphor';
    strength: number;  // 0.5-0.7
    preview: string;   // í•œ ì¤„ ìš”ì•½
  }[];
}
```

**ì‚¬ìš©ì ìƒí™©ì— ë”°ë¥¸ ê²½ë¡œ ì„ íƒ**:

| ìƒí™© | ê²½ë¡œ | íŠ¹ì§• | UI í‘œì‹œ |
|------|------|------|---------|
| ë©”ëª¨ ì‘ì„± ì¤‘ | ì£¼ë³€ ê²½ë¡œ | ë¹ ë¥¸ íŒíŠ¸, ê°€ë²¼ìš´ ì˜ê° | ì‚¬ì´ë“œë°”ì— 3-5ê°œ ë¯¸ë¦¬ë³´ê¸° |
| "ì—°ê²° ì°¾ê¸°" í´ë¦­ | ì¤‘ì‹¬ ê²½ë¡œ | ê¹Šì€ ë¶„ì„, ìƒì„¸í•œ ì„¤ëª… | ì „ì²´ í˜ì´ì§€, ê·¼ê±° ì œì‹œ |
| ê·¸ë˜í”„ íƒìƒ‰ ì¤‘ | í˜¼í•© | ì£¼ë³€ìœ¼ë¡œ ì‹œì‘ â†’ í´ë¦­ ì‹œ ì¤‘ì‹¬ | í˜¸ë²„: ìš”ì•½ / í´ë¦­: ìƒì„¸ |

---

## 1.3 ì–¸ì–´í•™ì  ì„¤ê³„

### ì›ë¦¬ 1: í”„ë ˆì„ ì˜ë¯¸ë¡  (Frame Semantics)

**í•µì‹¬ ê°œë…**: ë‹¨ì–´ëŠ” í”„ë ˆì„(ìƒí™© êµ¬ì¡°) ë‚´ì—ì„œ ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤.

**ì°½ì‹œì**: Charles Fillmore (1982)

**ì˜ˆì‹œ**: "êµ¬ë§¤" í”„ë ˆì„
```yaml
í”„ë ˆì„: ìƒì—…ì _ê±°ë˜
í•µì‹¬_ìš”ì†Œ:
  - êµ¬ë§¤ì (Buyer)
  - íŒë§¤ì (Seller)
  - ìƒí’ˆ (Goods)
  - ëŒ€ê°€ (Money)
  - ì‹œì  (Time)

ì—°ê²°_í”„ë ˆì„:
  - ì†Œìœ _ë³€ê²½
  - ê²½ì œ_êµí™˜
  - ì„ íƒ_í–‰ìœ„
```

**IdeaConnect ë°ì´í„° êµ¬ì¡° ì ìš©**:
```json
{
  "id": "quote_315",
  "content": "The best time to plant a tree was 20 years ago. The second best time is now.",
  "author": "Chinese Proverb",

  "linguistic": {
    "primary_frame": "ì‹œê°„ê³¼_í–‰ë™",
    "frame_elements": {
      "action": "ì‹¬ê¸° (ì‹ìˆ˜)",
      "optimal_time": "ê³¼ê±° (20ë…„ ì „)",
      "alternative_time": "í˜„ì¬",
      "implicit_message": "ì§€ê¸ˆ_ì‹œì‘í•˜ê¸°"
    },
    "frame_relations": [
      {
        "related_frame": "í›„íšŒì™€_íšŒë³µ",
        "relation": "inheritance"
      },
      {
        "related_frame": "ê¸°íšŒ_í¬ì°©",
        "relation": "subframe"
      }
    ]
  }
}
```

**í”„ë ˆì„ ê¸°ë°˜ ê²€ìƒ‰ì˜ ì¥ì **:
- "ë‚˜ë¬´ ì‹¬ê¸°"ë¥¼ ê²€ìƒ‰í•˜ì§€ ì•Šì•„ë„ "ì§€ê¸ˆ ì‹œì‘í•˜ê¸°" í”„ë ˆì„ìœ¼ë¡œ ì—°ê²°
- "í›„íšŒ"ì™€ "ê¸°íšŒ" ê°œë…ë„ ìë™ìœ¼ë¡œ ì—°ê²°
- ë§¥ë½ì  ì´í•´ ê°€ëŠ¥

---

### ì›ë¦¬ 2: ê°œë…ì  ì€ìœ  ì´ë¡  (Conceptual Metaphor Theory)

**í•µì‹¬ ê°œë…**: ì¶”ìƒì  ê°œë…ì€ êµ¬ì²´ì  ì€ìœ ë¡œ ì´í•´ëœë‹¤.

**ì°½ì‹œì**: George Lakoff, Mark Johnson (1980)

**í•µì‹¬ ì€ìœ  ì²´ê³„**:

#### ì€ìœ  1: "ì‹œê°„ì€ ìì›ì´ë‹¤"
```javascript
{
  system: "ì‹œê°„ì€_ìì›ì´ë‹¤",
  source_domain: "ë¬¼ì§ˆì _ìì›",
  target_domain: "ì‹œê°„",
  mappings: {
    spending: "ë³´ë‚´ë‹¤",
    saving: "ì•„ë¼ë‹¤",
    wasting: "ë‚­ë¹„í•˜ë‹¤",
    investing: "íˆ¬ìí•˜ë‹¤"
  },
  example_quotes: [
    "Time is money",
    "Don't waste your time",
    "Invest your time wisely"
  ]
}
```

#### ì€ìœ  2: "ì¸ìƒì€ ì—¬ì •ì´ë‹¤"
```javascript
{
  system: "ì¸ìƒì€_ì—¬ì •ì´ë‹¤",
  source_domain: "ë¬¼ë¦¬ì _ì—¬í–‰",
  target_domain: "ì¸ìƒ",
  mappings: {
    path: "ê¸¸/ê²½ë¡œ",
    obstacles: "ì¥ì• ë¬¼",
    destination: "ëª©í‘œ",
    companions: "ë™ë°˜ì",
    crossroads: "ì„ íƒì˜_ìˆœê°„"
  },
  example_quotes: [
    "Life is a journey, not a destination",
    "We're all on different paths",
    "Every setback is a setup for a comeback"
  ]
}
```

#### ì€ìœ  3: "ì•„ì´ë””ì–´ëŠ” ê±´ë¬¼ì´ë‹¤"
```javascript
{
  system: "ì•„ì´ë””ì–´ëŠ”_ê±´ë¬¼ì´ë‹¤",
  source_domain: "ê±´ì¶•",
  target_domain: "ì¶”ë¡ ",
  mappings: {
    foundation: "ê¸°ì´ˆ/ì „ì œ",
    structure: "ë…¼ë¦¬_êµ¬ì¡°",
    collapse: "ë…¼ë¦¬_ë¶•ê´´",
    support: "ê·¼ê±°"
  },
  example_quotes: [
    "Build your argument on solid foundations",
    "That theory doesn't hold up",
    "Strong evidence supports this claim"
  ]
}
```

**IdeaConnect ë°ì´í„°ì— ì€ìœ  íƒœê¹…**:
```json
{
  "content": "Build your dreams one brick at a time",
  "metaphor": {
    "system": "ëª©í‘œëŠ”_ê±´ë¬¼ì´ë‹¤",
    "elements": ["build", "brick", "one at a time"],
    "source_domain": "ê±´ì¶•",
    "target_domain": "ëª©í‘œ_ë‹¬ì„±",
    "related_metaphors": ["ì¸ìƒì€_ì—¬ì •ì´ë‹¤"]
  }
}
```

**ì€ìœ  ê¸°ë°˜ ì—°ê²°ì˜ í˜**:
- "ë²½ëŒ"ì„ ê²€ìƒ‰í•˜ì§€ ì•Šì•„ë„ "ìŠµê´€", "ê¾¸ì¤€í•¨"ê³¼ ì—°ê²°
- ì„œë¡œ ë‹¤ë¥¸ ì€ìœ  ì‹œìŠ¤í…œ ê°„ì˜ êµì°¨ ì—°ê²°
- ì°½ì˜ì  ì•„ì´ë””ì–´ ë°œê²¬

---

### ì›ë¦¬ 3: ì˜ë¯¸ ì—­í•  ì´ë¡  (Semantic Role Labeling)

**í•µì‹¬ ê°œë…**: ë¬¸ì¥ì˜ ì‹¬ì¸µ êµ¬ì¡°ë¥¼ í–‰ìœ„ì-í–‰ìœ„-ëŒ€ìƒìœ¼ë¡œ ë¶„í•´

**IdeaConnect ì ìš©**:
```python
{
  "content": "Courage is not the absence of fear, but the triumph over it.",
  "author": "Nelson Mandela",

  "semantic_roles": {
    "theme": "ìš©ê¸°",               # ì£¼ì œ
    "attribute": "ë‘ë ¤ì›€ì˜_ë¶€ì¬ê°€_ì•„ë‹˜",  # ì†ì„± (ë¶€ì •)
    "attribute_corrected": "ë‘ë ¤ì›€ì—_ëŒ€í•œ_ìŠ¹ë¦¬",  # ì‹¤ì œ ì†ì„±
    "implicit_agent": "ìš©ê¸°ìˆëŠ”_ì‚¬ëŒ"  # ì•”ë¬µì  í–‰ìœ„ì
  },

  "deep_structure": {
    "proposition": "ìš©ê¸° = ê·¹ë³µ(í–‰ìœ„ì, ë‘ë ¤ì›€)",
    "negation": "ìš©ê¸° â‰  ë¶€ì¬(ë‘ë ¤ì›€)"
  }
}
```

**ì˜ë¯¸ ì—­í•  ìë™ ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤**:
```python
import spacy

nlp = spacy.load("en_core_web_trf")

def extract_semantic_roles(text):
    doc = nlp(text)

    roles = {
        "agents": [],      # í–‰ìœ„ì
        "patients": [],    # í”¼í–‰ìœ„ì
        "themes": [],      # ì£¼ì œ
        "instruments": [], # ë„êµ¬
        "locations": [],   # ì¥ì†Œ
        "times": []        # ì‹œê°„
    }

    for token in doc:
        if token.dep_ == "nsubj":
            roles["agents"].append(token.text)
        elif token.dep_ == "dobj":
            roles["patients"].append(token.text)
        elif token.dep_ == "attr":
            roles["themes"].append(token.text)

    return roles
```

---

## 1.4 AI/NLP ê¸°ìˆ  ìŠ¤íƒ

### ê¸°ìˆ  1: ì„ë² ë”© ë²¡í„° ê³µê°„ (Embedding Space)

**ì‚¬ìš© ëª¨ë¸**: Sentence Transformers / OpenAI Embeddings

**ë‹¤ì¤‘ ì„ë² ë”© ì „ëµ**:
```python
from sentence_transformers import SentenceTransformer

class MultiEmbedding:
    def __init__(self):
        # 1. ì˜ë¯¸ì  ì„ë² ë”© (semantic)
        self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')

        # 2. ê°ì •ì  ì„ë² ë”© (emotional)
        self.emotion_model = SentenceTransformer('j-hartmann/emotion-english-distilroberta-base')

        # 3. ì˜ë„ ì„ë² ë”© (pragmatic)
        self.intent_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')

    def encode(self, text):
        return {
            'semantic': self.semantic_model.encode(text),    # 768-dim
            'emotional': self.emotion_model.encode(text),    # 768-dim
            'pragmatic': self.intent_model.encode(text)      # 768-dim
        }
```

**ë‹¤ì°¨ì› ìœ ì‚¬ë„ ê³„ì‚°**:
```python
from sklearn.metrics.pairwise import cosine_similarity

def multi_dimensional_similarity(vec1, vec2, weights):
    """
    weights ì˜ˆì‹œ:
    {
        'semantic': 0.5,    # ì˜ë¯¸ì  ìœ ì‚¬ë„ì— 50% ê°€ì¤‘ì¹˜
        'emotional': 0.3,   # ê°ì •ì  ìœ ì‚¬ë„ì— 30% ê°€ì¤‘ì¹˜
        'pragmatic': 0.2    # ì‹¤ìš©ì  ìœ ì‚¬ë„ì— 20% ê°€ì¤‘ì¹˜
    }
    """
    similarities = {}

    for dimension in ['semantic', 'emotional', 'pragmatic']:
        cos_sim = cosine_similarity(
            vec1[dimension].reshape(1, -1),
            vec2[dimension].reshape(1, -1)
        )[0][0]
        similarities[dimension] = cos_sim

    # ê°€ì¤‘ í‰ê· 
    weighted_sim = sum(
        similarities[dim] * weights[dim]
        for dim in weights
    )

    return weighted_sim, similarities
```

**ë°ì´í„° ì €ì¥ êµ¬ì¡°**:
```json
{
  "id": "node_001",
  "content": "We are what we repeatedly do.",
  "embeddings": {
    "semantic_v": [0.123, -0.456, 0.789, ...],   // 768 dim
    "emotional_v": [0.234, 0.567, -0.123, ...],  // 768 dim
    "pragmatic_v": [-0.345, 0.678, 0.234, ...]   // 768 dim
  },
  "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
  "version": "2.0"
}
```

---

### ê¸°ìˆ  2: ì§€ì‹ ê·¸ë˜í”„ ì„ë² ë”© (Knowledge Graph Embedding)

**ì‚¬ìš© ëª¨ë¸**: TransE, ComplEx, RotatE

**TransE ëª¨ë¸ ê°œë…**:
```
h + r â‰ˆ t
(head entity) + (relation) â‰ˆ (tail entity)

ì˜ˆì‹œ:
"ìŠµê´€" + "ê²°ê³¼ëŠ”" â‰ˆ "ì •ì²´ì„±"
```

**êµ¬í˜„ ì˜ˆì‹œ**:
```python
import numpy as np

class TransE:
    """
    TransE: Translating Embeddings for Knowledge Graphs
    """

    def __init__(self, entity_dim=128, relation_dim=128):
        self.entity_embeddings = {}
        self.relation_embeddings = {}
        self.entity_dim = entity_dim
        self.relation_dim = relation_dim

    def train(self, triples, epochs=100, learning_rate=0.01):
        """
        triples: [("ìŠµê´€", "leads_to", "ì •ì²´ì„±"), ...]
        """

        # ì„ë² ë”© ì´ˆê¸°í™”
        entities = set()
        relations = set()

        for (h, r, t) in triples:
            entities.add(h)
            entities.add(t)
            relations.add(r)

        for entity in entities:
            self.entity_embeddings[entity] = np.random.randn(self.entity_dim)

        for relation in relations:
            self.relation_embeddings[relation] = np.random.randn(self.relation_dim)

        # í•™ìŠµ
        for epoch in range(epochs):
            for (h, r, t) in triples:
                h_vec = self.entity_embeddings[h]
                r_vec = self.relation_embeddings[r]
                t_vec = self.entity_embeddings[t]

                # ì†ì‹¤: ||h + r - t||
                loss = np.linalg.norm(h_vec + r_vec - t_vec)

                # ê·¸ë˜ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸ (ê°„ë‹¨íˆ í‘œí˜„)
                grad = (h_vec + r_vec - t_vec) / (loss + 1e-8)

                self.entity_embeddings[h] -= learning_rate * grad
                self.relation_embeddings[r] -= learning_rate * grad
                self.entity_embeddings[t] += learning_rate * grad

    def predict_tail(self, head, relation):
        """ì£¼ì–´ì§„ (head, relation)ìœ¼ë¡œ tail ì˜ˆì¸¡"""
        h_vec = self.entity_embeddings[head]
        r_vec = self.relation_embeddings[relation]

        predicted_t = h_vec + r_vec

        # ê°€ì¥ ê°€ê¹Œìš´ entity ì°¾ê¸°
        min_distance = float('inf')
        best_entity = None

        for entity, vec in self.entity_embeddings.items():
            distance = np.linalg.norm(predicted_t - vec)
            if distance < min_distance:
                min_distance = distance
                best_entity = entity

        return best_entity, min_distance

# ì‚¬ìš© ì˜ˆì‹œ
model = TransE()
triples = [
    ("ìŠµê´€", "ê²°ê³¼ëŠ”", "ì •ì²´ì„±"),
    ("ë°˜ë³µ", "leads_to", "ìŠµê´€"),
    ("ì‹œê°„", "enables", "ë³€í™”"),
    ("ì˜ì§€", "requires", "ë™ê¸°")
]

model.train(triples)

# "ìŠµê´€" + "ê²°ê³¼ëŠ”" â†’ ?
result, confidence = model.predict_tail("ìŠµê´€", "ê²°ê³¼ëŠ”")
print(f"ì˜ˆì¸¡: {result}, ì‹ ë¢°ë„: {1 - confidence}")  # â†’ "ì •ì²´ì„±"
```

---

### ê¸°ìˆ  3: ê´€ê³„ ì¶”ì¶œ (Relation Extraction)

**ê´€ê³„ ì˜¨í†¨ë¡œì§€ (Ontology)**:
```yaml
ê´€ê³„_ì²´ê³„:
  ì¸ê³¼_ê´€ê³„:
    - causes: Aê°€ Bë¥¼ ì•¼ê¸°í•¨
    - enables: Aê°€ Bë¥¼ ê°€ëŠ¥í•˜ê²Œ í•¨
    - prevents: Aê°€ Bë¥¼ ë§‰ìŒ
    - requires: Aê°€ Bë¥¼ í•„ìš”ë¡œ í•¨

  êµ¬ì¡°_ê´€ê³„:
    - part_of: AëŠ” Bì˜ ë¶€ë¶„
    - instance_of: AëŠ” Bì˜ ì‚¬ë¡€
    - contrasts_with: AëŠ” Bì™€ ëŒ€ì¡°ë¨

  ì‹œê°„_ê´€ê³„:
    - precedes: Aê°€ Bë³´ë‹¤ ë¨¼ì €
    - follows: Aê°€ Bë¥¼ ë”°ë¦„
    - during: AëŠ” B ë™ì•ˆ

  ì˜ë¯¸_ê´€ê³„:
    - similar_to: ìœ ì‚¬í•¨
    - analogous_to: ìœ ì¶” ê°€ëŠ¥
    - metaphor_of: ì€ìœ  ê´€ê³„

  ê°ì •_ê´€ê³„:
    - evokes: Aê°€ B ê°ì •ì„ ìœ ë°œ
    - expressed_by: AëŠ” Bë¡œ í‘œí˜„ë¨
```

**ìë™ ê´€ê³„ ì¶”ì¶œ êµ¬í˜„**:
```python
import spacy
from transformers import pipeline

nlp = spacy.load("en_core_web_trf")

def extract_relations(quote1, quote2):
    """ë‘ ëª…ì–¸ ê°„ ê´€ê³„ ìë™ ì¶”ì¶œ"""

    doc1 = nlp(quote1.content)
    doc2 = nlp(quote2.content)

    relations = []

    # 1. ê³µí†µ ê°œë… ì¶”ì¶œ
    concepts1 = [ent.text for ent in doc1.ents]
    concepts2 = [ent.text for ent in doc2.ents]
    shared = set(concepts1) & set(concepts2)

    # 2. ì˜ë¯¸ì  ìœ ì‚¬ë„
    similarity = doc1.similarity(doc2)

    if similarity > 0.8:
        relations.append({
            "type": "similar_to",
            "strength": similarity,
            "evidence": f"ì½”ì‚¬ì¸ ìœ ì‚¬ë„ {similarity:.2f}"
        })

    # 3. ì¸ê³¼ ê´€ê³„ ë§ˆì»¤ ê°ì§€
    causal_markers = ["because", "therefore", "thus", "leads to", "results in"]

    if any(marker in quote1.content.lower() for marker in causal_markers):
        if any(marker in quote2.content.lower() for marker in causal_markers):
            relations.append({
                "type": "causal_chain",
                "strength": 0.7,
                "evidence": "ì¸ê³¼ ê´€ê³„ ë§ˆì»¤ ë°œê²¬"
            })

    # 4. ëŒ€ì¡° ê´€ê³„ ë§ˆì»¤
    contrast_markers = ["but", "however", "although", "while", "whereas"]

    if any(marker in quote1.content.lower() for marker in contrast_markers):
        relations.append({
            "type": "contrasts_with",
            "strength": 0.6,
            "evidence": "ëŒ€ì¡° ê´€ê³„ ë§ˆì»¤ ë°œê²¬"
        })

    return relations
```

---

## 1.5 ì´ë¡ ì  ê¸°ë°˜ ìš”ì•½

### í•µì‹¬ ì›ë¦¬ í†µí•©

| í•™ë¬¸ ë¶„ì•¼ | í•µì‹¬ ì´ë¡  | IdeaConnect ì ìš© | íš¨ê³¼ |
|---------|---------|----------------|------|
| **ì¸ì§€ì‹¬ë¦¬í•™** | ìŠ¤í‚¤ë§ˆ ì´ë¡  | ê°œë…ì˜ êµ¬ì¡°í™”ëœ ì €ì¥ | ë§¥ë½ì  ì—°ê²° |
| **ì¸ì§€ì‹¬ë¦¬í•™** | í™•ì‚° í™œì„±í™” | ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì „íŒŒ | ì—°ì‡„ì  ë°œê²¬ |
| **ì¸ì§€ì‹¬ë¦¬í•™** | ELM | ì´ì¤‘ ê²€ìƒ‰ ê²½ë¡œ | ìƒí™©ë³„ ìµœì í™” |
| **ì–¸ì–´í•™** | í”„ë ˆì„ ì˜ë¯¸ë¡  | ìƒí™© êµ¬ì¡° ë§¤í•‘ | ê¹Šì€ ì´í•´ |
| **ì–¸ì–´í•™** | ê°œë…ì  ì€ìœ  | ì€ìœ  ì‹œìŠ¤í…œ íƒœê¹… | ì°½ì˜ì  ì—°ê²° |
| **ì–¸ì–´í•™** | ì˜ë¯¸ ì—­í•  | ì‹¬ì¸µ êµ¬ì¡° ë¶„ì„ | ì •í™•í•œ ê´€ê³„ |
| **AI/NLP** | ì„ë² ë”© ê³µê°„ | ë‹¤ì°¨ì› ë²¡í„° | ìˆ˜ì¹˜ì  ìœ ì‚¬ë„ |
| **AI/NLP** | ì§€ì‹ ê·¸ë˜í”„ ì„ë² ë”© | TransE ëª¨ë¸ | ê´€ê³„ ì˜ˆì¸¡ |
| **AI/NLP** | ê´€ê³„ ì¶”ì¶œ | ìë™ ì˜¨í†¨ë¡œì§€ | ëŒ€ê·œëª¨ ì²˜ë¦¬ |

### ì°¨ë³„í™” í¬ì¸íŠ¸

**ê¸°ì¡´ ì ‘ê·¼ vs IdeaConnect v2.0:**

| í•­ëª© | ê¸°ì¡´ | IdeaConnect v2.0 |
|------|------|-----------------|
| ê²€ìƒ‰ ë°©ì‹ | í‚¤ì›Œë“œ ë§¤ì¹­ | ë‹¤ì°¨ì› ì„ë² ë”© (ì˜ë¯¸Â·ê°ì •Â·ì‹¤ìš©) |
| ë¶„ë¥˜ | ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ | ë‹¤ì¤‘ ìŠ¤í‚¤ë§ˆ + í”„ë ˆì„ |
| ê´€ê³„ | ì •ì  | ë™ì Â·ê°€ì¤‘Â·ë§¥ë½ì  |
| ì‚¬ìš©ì ê²½í—˜ | ê²€ìƒ‰ ì¤‘ì‹¬ | ë°œê²¬(discovery) ì¤‘ì‹¬ |
| ì¶”ì²œ | ì¼ë¥ ì  | ê°œì¸í™”Â·ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ |
| ì‹œê°í™” | 2D ë¦¬ìŠ¤íŠ¸ | 3D ì§€ì‹ ê·¸ë˜í”„ |
| í’ˆì§ˆ ê´€ë¦¬ | ì•Œê³ ë¦¬ì¦˜ë§Œ | ì•Œê³ ë¦¬ì¦˜ + ì „ë¬¸ê°€ + ì»¤ë®¤ë‹ˆí‹° |

---

# Part 2: ë°ì´í„° ì•„í‚¤í…ì²˜

## 2.1 í–¥ìƒëœ ë…¸ë“œ(Node) êµ¬ì¡°

### ë‹¤ì¸µì  í‘œí˜„ ì‹œìŠ¤í…œ

**ì„¤ê³„ ì² í•™**: í•˜ë‚˜ì˜ ì•„ì´ë””ì–´ë¥¼ ë‹¤ì–‘í•œ ê°ë„ì—ì„œ ë¶„ì„í•˜ê³  ì €ì¥

```typescript
interface IdeaNode {
  // ===== ê¸°ë³¸ ì‹ë³„ ì •ë³´ =====
  id: string;                    // UUID
  content: string;               // ì›ë¬¸ (ì˜ì–´)
  content_ko?: string;           // ë²ˆì—­ (í•œêµ­ì–´)
  created_at: Date;
  updated_at: Date;

  // ===== ì¶œì²˜ ë©”íƒ€ë°ì´í„° =====
  source: {
    author: string;              // ì €ì/ì¸ë¬¼
    author_ko?: string;          // ì €ì í•œêµ­ì–´ëª…
    work?: string;               // ì‘í’ˆëª…
    work_ko?: string;
    year?: number;               // ì—°ë„
    url?: string;                // ì›¹ ì¶œì²˜
    isbn?: string;               // ì±…
    doi?: string;                // ë…¼ë¬¸
    imdb_id?: string;            // ì˜í™”
    verified: boolean;           // ì¶œì²˜ ê²€ì¦ ì—¬ë¶€
  };

  // ===== ì–¸ì–´í•™ì  ë¶„ì„ =====
  linguistic: {
    // í”„ë ˆì„ ì˜ë¯¸ë¡ 
    primary_frame: string;       // "ì‹œê°„ê³¼_í–‰ë™"
    frame_elements: Record<string, string>;
    related_frames: {
      frame: string;
      relation: 'inheritance' | 'subframe' | 'uses' | 'perspective';
    }[];

    // ì€ìœ  ë¶„ì„
    metaphors: {
      system: string;            // "ì‹œê°„ì€_ìì›ì´ë‹¤"
      elements: string[];        // ["spending", "saving"]
      source_domain: string;     // "ë¬¼ì§ˆì _ìì›"
      target_domain: string;     // "ì‹œê°„"
      strength: number;          // 0.0-1.0
    }[];

    // ì˜ë¯¸ ì—­í• 
    semantic_roles: {
      theme?: string;            // ì£¼ì œ
      agent?: string;            // í–‰ìœ„ì
      patient?: string;          // í”¼í–‰ìœ„ì
      instrument?: string;       // ë„êµ¬
      location?: string;         // ì¥ì†Œ
      time?: string;             // ì‹œê°„
    };

    // í™”í–‰ ì´ë¡  (Speech Act)
    speech_act: 'assertive' | 'directive' | 'commissive' | 'expressive' | 'declarative';

    // ë¶€ì • í‘œí˜„ ë¶„ì„
    negation?: {
      has_negation: boolean;
      negated_concept: string;
      affirmative_alternative?: string;
    };
  };

  // ===== ì¸ì§€ì‹¬ë¦¬í•™ì  ë¶„ì„ =====
  cognitive: {
    // ìŠ¤í‚¤ë§ˆ ë§¤í•‘
    schemas: {
      type: string;              // "ìŠµê´€ í˜•ì„± ìŠ¤í‚¤ë§ˆ"
      slot: string;              // "ê²°ê³¼"
      related_slots: Record<string, string[]>;
      activation_strength: number; // 0.0-1.0
    }[];

    // ì •ë³´ ì²˜ë¦¬ ìˆ˜ì¤€
    processing_level: 'surface' | 'semantic' | 'pragmatic';

    // ì¸ì§€ ë¶€í•˜
    cognitive_load: {
      level: 'low' | 'medium' | 'high';
      complexity_score: number;  // 0-100
      abstractness: number;      // 0.0-1.0 (êµ¬ì²´ì  vs ì¶”ìƒì )
    };

    // ê¸°ì–µ ì¸ì¶œ ë‹¨ì„œ
    retrieval_cues: string[];    // ["ìŠµê´€", "ë°˜ë³µ", "ì •ì²´ì„±"]

    // ì •êµí™” ìˆ˜ì¤€
    elaboration: {
      level: 'basic' | 'intermediate' | 'advanced';
      requires_context: boolean;
    };
  };

  // ===== ê°ì •/íƒœë„ ë¶„ì„ =====
  affective: {
    // ê°ì • ë²¡í„° (Plutchikì˜ 8ê°€ì§€ ê¸°ë³¸ ê°ì •)
    emotions: {
      joy: number;               // 0.0-1.0
      trust: number;
      fear: number;
      surprise: number;
      sadness: number;
      disgust: number;
      anger: number;
      anticipation: number;
    };

    // ê°ì •ê°€ (Valence): ê¸ì •/ë¶€ì •
    valence: number;             // -1.0 (ë§¤ìš° ë¶€ì •) ~ +1.0 (ë§¤ìš° ê¸ì •)

    // ê°ì„±ë„ (Arousal): ì°¨ë¶„í•¨/í¥ë¶„
    arousal: number;             // 0.0 (ì°¨ë¶„) ~ 1.0 (í¥ë¶„)

    // ì§€ë°°ì„± (Dominance): í†µì œê°
    dominance: number;           // 0.0 (ìˆ˜ë™ì ) ~ 1.0 (ì§€ë°°ì )

    // ê°ì • ê°•ë„
    intensity: number;           // 0.0-1.0

    // ì£¼ìš” ê°ì • (ìë™ ê³„ì‚°)
    primary_emotion: string;     // "í¬ë§", "ë™ê¸°ë¶€ì—¬", "ì„±ì°°" ë“±
  };

  // ===== ì‹¤ìš©ì  ì°¨ì› =====
  pragmatic: {
    // ì ìš© ê°€ëŠ¥í•œ ìƒí™©/ë§¥ë½
    applicable_contexts: string[]; // ["ìê¸°ê³„ë°œ", "ìŠµê´€ í˜•ì„±", "ë™ê¸°ë¶€ì—¬"]

    // í–‰ë™ ìœ ë„ì„± (Affordance)
    action_tendencies: string[]; // ["ë°˜ë³µ ì‹¤ì²œ", "ìê¸° ì„±ì°°", "ëª©í‘œ ì„¤ì •"]

    // ì‹¤ì²œ ë‚œì´ë„
    implementation: {
      difficulty: 'easy' | 'medium' | 'hard';
      time_required: 'immediate' | 'days' | 'weeks' | 'months' | 'years';
      resources_needed: string[];
    };

    // ì‹œê°„ ì§€í‰
    time_horizon: 'immediate' | 'short-term' | 'long-term' | 'lifelong';

    // ëŒ€ìƒ ì²­ì¤‘
    target_audience: string[];   // ["í•™ìƒ", "ì§ì¥ì¸", "ì°½ì—…ê°€", "ì¼ë°˜"]
  };

  // ===== ë²¡í„° ì„ë² ë”© =====
  embeddings: {
    semantic: number[];          // 768-dim (ì˜ë¯¸ì  ìœ ì‚¬ë„)
    emotional: number[];         // 768-dim (ê°ì •ì  ìœ ì‚¬ë„)
    pragmatic: number[];         // 768-dim (ì‹¤ìš©ì  ìœ ì‚¬ë„)
    kg_embedding?: number[];     // 128-dim (ì§€ì‹ ê·¸ë˜í”„)

    // ì„ë² ë”© ë©”íƒ€ì •ë³´
    model_version: string;       // "all-MiniLM-L6-v2"
    generated_at: Date;
  };

  // ===== ë¶„ë¥˜ ì •ë³´ =====
  classification: {
    // ê¸°ë³¸ ì¹´í…Œê³ ë¦¬
    primary_category: IdeaType;  // "famous-quote", "book", etc.
    secondary_categories: IdeaType[];

    // ì£¼ì œ íƒœê·¸
    topics: string[];            // ["ìŠµê´€", "ì„±ì¥", "ì‹œê°„ê´€ë¦¬"]

    // í‚¤ì›Œë“œ (ê²€ìƒ‰ìš©)
    keywords: string[];          // ìë™ ì¶”ì¶œ + ìˆ˜ë™ íë ˆì´ì…˜

    // ë‚œì´ë„
    difficulty_level: 'beginner' | 'intermediate' | 'advanced';
  };

  // ===== í’ˆì§ˆ & í†µê³„ =====
  quality: {
    // ìë™ í’ˆì§ˆ ì ìˆ˜
    auto_score: number;          // 0-100

    // ì „ë¬¸ê°€ íë ˆì´ì…˜
    curated: boolean;
    curator_notes?: string;

    // ì™„ì„±ë„
    completeness: {
      has_embeddings: boolean;
      has_linguistic: boolean;
      has_cognitive: boolean;
      has_affective: boolean;
      overall: number;           // 0-100
    };
  };

  stats: {
    view_count: number;
    connection_count: number;    // ëª‡ ê°œì˜ ì—£ì§€ì™€ ì—°ê²°ë˜ì–´ ìˆëŠ”ê°€
    user_saved_count: number;    // ëª‡ ëª…ì´ ì €ì¥í–ˆëŠ”ê°€
    avg_rating: number;          // 0-5
    last_accessed: Date;
  };
}

// ì•„ì´ë””ì–´ íƒ€ì… ì •ì˜
type IdeaType =
  | 'famous-quote'
  | 'book'
  | 'proverb'
  | 'movie'
  | 'drama'
  | 'animation'
  | 'academic'
  | 'web'
  | 'essay'
  | 'poem';
```

### ë…¸ë“œ êµ¬ì¡° ì„¤ê³„ ì˜ë„

| ì„¹ì…˜ | ëª©ì  | í™œìš© |
|------|------|------|
| **ê¸°ë³¸ ì‹ë³„** | ê³ ìœ ì„±, ë²„ì „ ê´€ë¦¬ | CRUD ì‘ì—… |
| **ì¶œì²˜ ë©”íƒ€ë°ì´í„°** | ì‹ ë¢°ì„±, ì¶”ì ì„± | ê²€ì¦, ì¸ìš© |
| **ì–¸ì–´í•™ì  ë¶„ì„** | ê¹Šì€ ì˜ë¯¸ ì´í•´ | í”„ë ˆì„ ê¸°ë°˜ ê²€ìƒ‰, ì€ìœ  ì—°ê²° |
| **ì¸ì§€ì‹¬ë¦¬í•™ì  ë¶„ì„** | ì‚¬ê³  êµ¬ì¡° íŒŒì•… | ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ ì—°ê²°, í™œì„±í™” ì „íŒŒ |
| **ê°ì •/íƒœë„ ë¶„ì„** | ê°ì •ì  ê³µëª… | ê°ì • ê¸°ë°˜ ì¶”ì²œ, ë¬´ë“œë³„ íƒìƒ‰ |
| **ì‹¤ìš©ì  ì°¨ì›** | ì ìš© ê°€ëŠ¥ì„± | ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ì¶”ì²œ |
| **ë²¡í„° ì„ë² ë”©** | ìˆ˜ì¹˜ì  ìœ ì‚¬ë„ | ë¹ ë¥¸ ê²€ìƒ‰, í´ëŸ¬ìŠ¤í„°ë§ |
| **ë¶„ë¥˜ ì •ë³´** | ì¡°ì§í™” | í•„í„°ë§, íƒìƒ‰ |
| **í’ˆì§ˆ & í†µê³„** | ì‹ ë¢°ì„±, ì¸ê¸°ë„ | ë­í‚¹, íë ˆì´ì…˜ |

---

## 2.2 ë‹¤ì°¨ì› ì—£ì§€(Edge) êµ¬ì¡°

### ê´€ê³„ì˜ ë³µì¡ì„± í‘œí˜„

**ì„¤ê³„ ì² í•™**: ë‘ ì•„ì´ë””ì–´ ê°„ì˜ ê´€ê³„ëŠ” ë‹¨ìˆœí•œ "ì—°ê²°ë¨"ì´ ì•„ë‹Œ ë‹¤ì¸µì  ì˜ë¯¸

```typescript
interface IdeaEdge {
  // ===== ê¸°ë³¸ ì‹ë³„ =====
  id: string;                    // UUID
  from: string;                  // ì¶œë°œ ë…¸ë“œ ID
  to: string;                    // ë„ì°© ë…¸ë“œ ID
  created_at: Date;

  // ===== ê´€ê³„ ìœ í˜• =====
  relation_type: RelationType;

  // ===== ê´€ê³„ ê°•ë„ =====
  strength: number;              // 0.0-1.0 (ì¢…í•© ê°•ë„)

  // ===== ì°¨ì›ë³„ ì ìˆ˜ =====
  dimensions: {
    semantic_similarity: number;     // ì˜ë¯¸ì  ìœ ì‚¬ì„± (0-1)
    emotional_resonance: number;     // ê°ì •ì  ê³µëª… (0-1)
    pragmatic_alignment: number;     // ì‹¤ìš©ì  ì •ë ¬ (0-1)
    metaphorical_connection: number; // ì€ìœ ì  ì—°ê²° (0-1)
    causal_strength: number;         // ì¸ê³¼ ê´€ê³„ ê°•ë„ (0-1)
    temporal_proximity: number;      // ì‹œê°„ì  ê·¼ì ‘ì„± (0-1)
  };

  // ===== ê´€ê³„ ì„¤ëª… =====
  reasoning: {
    automatic: string;           // AI ìë™ ìƒì„±
    curated?: string;            // íë ˆì´í„° ìˆ˜ë™ ì‘ì„±
    evidence: string[];          // ê·¼ê±° ë¦¬ìŠ¤íŠ¸
  };

  // ===== ì–‘ë°©í–¥ì„± =====
  bidirectional: boolean;        // ì–‘ë°©í–¥ ê´€ê³„ì¸ê°€?
  reverse_relation?: RelationType; // ì—­ë°©í–¥ ê´€ê³„ ìœ í˜•

  // ===== ì»¨í…ìŠ¤íŠ¸ ì˜ì¡´ì„± =====
  context_dependent: boolean;    // íŠ¹ì • ë§¥ë½ì—ì„œë§Œ ìœ íš¨í•œê°€?
  contexts?: string[];           // ["ìê¸°ê³„ë°œ", "ë¦¬ë”ì‹­"]

  // ===== ì‹ ë¢°ë„ & ì¶œì²˜ =====
  confidence: number;            // 0.0-1.0
  source: 'algorithm' | 'expert' | 'community' | 'user';

  // ===== í™œì„±í™” ì´ë ¥ =====
  activation: {
    count: number;               // ëª‡ ë²ˆ í™œì„±í™”ë˜ì—ˆëŠ”ê°€
    last_activated: Date;
    decay_rate: number;          // í™•ì‚° í™œì„±í™” ê°ì‡ ìœ¨
  };

  // ===== ê²€ì¦ & í’ˆì§ˆ =====
  verified: boolean;             // ì „ë¬¸ê°€ ê²€ì¦ ì—¬ë¶€
  quality_score: number;         // 0-100

  // ===== ë©”íƒ€ì •ë³´ =====
  version: string;
  notes?: string;
}

// ê´€ê³„ ìœ í˜• ì˜¨í†¨ë¡œì§€
type RelationType =
  // === ì˜ë¯¸ì  ê´€ê³„ ===
  | 'similar_to'              // ìœ ì‚¬í•¨
  | 'opposite_to'             // ë°˜ëŒ€ë¨
  | 'part_of'                 // ë¶€ë¶„-ì „ì²´
  | 'example_of'              // ì‚¬ë¡€
  | 'generalizes_to'          // ì¼ë°˜í™”
  | 'specializes_to'          // íŠ¹ìˆ˜í™”

  // === ì¸ê³¼ì  ê´€ê³„ ===
  | 'causes'                  // Aê°€ Bë¥¼ ì•¼ê¸°
  | 'enables'                 // Aê°€ Bë¥¼ ê°€ëŠ¥í•˜ê²Œ í•¨
  | 'prevents'                // Aê°€ Bë¥¼ ë§‰ìŒ
  | 'requires'                // Aê°€ Bë¥¼ í•„ìš”ë¡œ í•¨
  | 'contributes_to'          // Aê°€ Bì— ê¸°ì—¬

  // === ì‹œê°„ì  ê´€ê³„ ===
  | 'precedes'                // Aê°€ Bë³´ë‹¤ ë¨¼ì €
  | 'follows'                 // Aê°€ Bë¥¼ ë”°ë¦„
  | 'concurrent_with'         // Aì™€ Bê°€ ë™ì‹œ

  // === ë…¼ë¦¬ì  ê´€ê³„ ===
  | 'supports'                // Aê°€ Bë¥¼ ì§€ì§€
  | 'contradicts'             // Aê°€ Bì™€ ëª¨ìˆœ
  | 'refines'                 // Aê°€ Bë¥¼ ì •ì œ
  | 'extends'                 // Aê°€ Bë¥¼ í™•ì¥
  | 'implies'                 // Aê°€ Bë¥¼ í•¨ì˜

  // === ì€ìœ ì  ê´€ê³„ ===
  | 'metaphor_of'             // AëŠ” Bì˜ ì€ìœ 
  | 'analogy_to'              // AëŠ” Bì™€ ìœ ì¶”

  // === ê°ì •ì  ê´€ê³„ ===
  | 'evokes_same_emotion'     // ê°™ì€ ê°ì • ìœ ë°œ
  | 'contrasting_emotion'     // ëŒ€ì¡°ë˜ëŠ” ê°ì •

  // === ì‹¤ìš©ì  ê´€ê³„ ===
  | 'implements_same_principle' // ê°™ì€ ì›ë¦¬ êµ¬í˜„
  | 'alternative_approach'    // ëŒ€ì•ˆì  ì ‘ê·¼
  | 'complements'             // ë³´ì™„ ê´€ê³„

  // === êµ¬ì¡°ì  ê´€ê³„ ===
  | 'belongs_to_category'     // ì¹´í…Œê³ ë¦¬ ì†Œì†
  | 'shares_frame'            // í”„ë ˆì„ ê³µìœ 
  | 'shares_metaphor';        // ì€ìœ  ê³µìœ 
```

### ì—£ì§€ ìë™ ìƒì„± ì•Œê³ ë¦¬ì¦˜

```python
def create_edge_automatically(node1: IdeaNode, node2: IdeaNode) -> IdeaEdge | None:
    """
    ë‘ ë…¸ë“œ ê°„ ì—£ì§€ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±
    """

    # 1. ì°¨ì›ë³„ ìœ ì‚¬ë„ ê³„ì‚°
    dimensions = calculate_multi_dimensional_similarity(node1, node2)

    # 2. ì¢…í•© ê°•ë„ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
    strength = (
        dimensions['semantic_similarity'] * 0.35 +
        dimensions['emotional_resonance'] * 0.25 +
        dimensions['pragmatic_alignment'] * 0.20 +
        dimensions['metaphorical_connection'] * 0.15 +
        dimensions['causal_strength'] * 0.05
    )

    # 3. ì„ê³„ê°’ ì²´í¬ (0.65 ì´ìƒë§Œ ì—£ì§€ ìƒì„±)
    if strength < 0.65:
        return None

    # 4. ê´€ê³„ ìœ í˜• ì¶”ë¡ 
    relation_type = infer_relation_type(node1, node2, dimensions)

    # 5. ì–‘ë°©í–¥ì„± íŒë‹¨
    bidirectional = is_bidirectional(relation_type)

    # 6. ì„¤ëª… ìƒì„±
    reasoning = generate_reasoning(node1, node2, relation_type, dimensions)

    # 7. ì—£ì§€ ê°ì²´ ìƒì„±
    edge = IdeaEdge(
        id=generate_uuid(),
        from=node1.id,
        to=node2.id,
        relation_type=relation_type,
        strength=strength,
        dimensions=dimensions,
        reasoning=reasoning,
        bidirectional=bidirectional,
        confidence=calculate_confidence(dimensions),
        source='algorithm',
        created_at=datetime.now()
    )

    return edge

def infer_relation_type(node1, node2, dimensions) -> RelationType:
    """ê´€ê³„ ìœ í˜• ìë™ ì¶”ë¡ """

    # ì˜ë¯¸ì  ìœ ì‚¬ë„ê°€ ë§¤ìš° ë†’ìœ¼ë©´
    if dimensions['semantic_similarity'] > 0.85:
        return 'similar_to'

    # ê°ì •ì´ ë¹„ìŠ·í•˜ë©´
    if dimensions['emotional_resonance'] > 0.8:
        return 'evokes_same_emotion'

    # ê°™ì€ ì€ìœ  ì‹œìŠ¤í…œì„ ê³µìœ í•˜ë©´
    if dimensions['metaphorical_connection'] > 0.8:
        return 'shares_metaphor'

    # ì¸ê³¼ ê´€ê³„ ë§ˆì»¤ê°€ ìˆìœ¼ë©´
    if has_causal_markers(node1, node2):
        return 'causes' if dimensions['causal_strength'] > 0.7 else 'contributes_to'

    # í”„ë ˆì„ì´ ê°™ìœ¼ë©´
    if shares_frame(node1, node2):
        return 'shares_frame'

    # ê¸°ë³¸ê°’
    return 'similar_to'
```

---

## 2.3 ì„ë² ë”© ì „ëµ

### ë‹¤ì¤‘ ì„ë² ë”© ì‹œìŠ¤í…œ

**ëª©ì **: ì„œë¡œ ë‹¤ë¥¸ ì°¨ì›ì˜ ìœ ì‚¬ë„ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ê³„ì‚°

```python
# scripts/embeddings/multi_embedding.py

from sentence_transformers import SentenceTransformer
import numpy as np
from typing import Dict, List
import pickle

class MultiEmbeddingSystem:
    """ë‹¤ì¤‘ ì„ë² ë”© ìƒì„± ë° ê´€ë¦¬"""

    def __init__(self):
        print("ğŸ”§ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")

        # 1. ì˜ë¯¸ì  ì„ë² ë”©
        self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')
        print("âœ… Semantic ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        # 2. ê°ì •ì  ì„ë² ë”©
        self.emotion_model = SentenceTransformer('j-hartmann/emotion-english-distilroberta-base')
        print("âœ… Emotion ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        # 3. ë‹¤êµ­ì–´ ì„ë² ë”© (í•œêµ­ì–´ ì§€ì›)
        self.multilingual_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        print("âœ… Multilingual ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        # ìºì‹œ
        self.cache = {}

    def encode_node(self, node: IdeaNode) -> Dict[str, np.ndarray]:
        """ë…¸ë“œë¥¼ ë‹¤ì¤‘ ì„ë² ë”©ìœ¼ë¡œ ì¸ì½”ë”©"""

        # ìºì‹œ ì²´í¬
        if node.id in self.cache:
            return self.cache[node.id]

        content = node.content
        content_ko = node.content_ko or content

        # ì„ë² ë”© ìƒì„±
        embeddings = {
            'semantic': self.semantic_model.encode(content),
            'emotional': self.emotion_model.encode(content),
            'multilingual': self.multilingual_model.encode(content_ko)
        }

        # ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ìµœì í™”)
        for key in embeddings:
            embeddings[key] = embeddings[key] / np.linalg.norm(embeddings[key])

        # ìºì‹œ ì €ì¥
        self.cache[node.id] = embeddings

        return embeddings

    def batch_encode(self, nodes: List[IdeaNode]) -> Dict[str, List[np.ndarray]]:
        """ë°°ì¹˜ ì¸ì½”ë”© (íš¨ìœ¨ì )"""

        contents = [node.content for node in nodes]
        contents_ko = [node.content_ko or node.content for node in nodes]

        # ë°°ì¹˜ ì²˜ë¦¬
        semantic_batch = self.semantic_model.encode(contents, show_progress_bar=True)
        emotional_batch = self.emotion_model.encode(contents, show_progress_bar=True)
        multilingual_batch = self.multilingual_model.encode(contents_ko, show_progress_bar=True)

        # ì •ê·œí™”
        semantic_batch = semantic_batch / np.linalg.norm(semantic_batch, axis=1, keepdims=True)
        emotional_batch = emotional_batch / np.linalg.norm(emotional_batch, axis=1, keepdims=True)
        multilingual_batch = multilingual_batch / np.linalg.norm(multilingual_batch, axis=1, keepdims=True)

        return {
            'semantic': semantic_batch,
            'emotional': emotional_batch,
            'multilingual': multilingual_batch
        }

    def save_cache(self, filepath: str):
        """ìºì‹œ ì €ì¥"""
        with open(filepath, 'wb') as f:
            pickle.dump(self.cache, f)
        print(f"ğŸ’¾ ìºì‹œ ì €ì¥ ì™„ë£Œ: {filepath}")

    def load_cache(self, filepath: str):
        """ìºì‹œ ë¡œë“œ"""
        with open(filepath, 'rb') as f:
            self.cache = pickle.load(f)
        print(f"ğŸ“‚ ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(self.cache)}ê°œ í•­ëª©")
```

### ë²¡í„° ì¸ë±ì‹± (FAISS)

**ëª©ì **: 50,000ê°œ ë…¸ë“œì—ì„œ ë¹ ë¥¸ ìœ ì‚¬ë„ ê²€ìƒ‰

```python
# scripts/embeddings/vector_index.py

import faiss
import numpy as np
from typing import List, Tuple

class VectorIndex:
    """FAISS ê¸°ë°˜ ë²¡í„° ì¸ë±ìŠ¤"""

    def __init__(self, dimension: int = 768):
        self.dimension = dimension

        # FAISS ì¸ë±ìŠ¤ ìƒì„± (Inner Product = ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
        self.index = faiss.IndexFlatIP(dimension)

        # ID ë§¤í•‘
        self.id_to_idx = {}  # node_id -> index
        self.idx_to_id = {}  # index -> node_id

        self.next_idx = 0

    def add_vector(self, node_id: str, vector: np.ndarray):
        """ë²¡í„° ì¶”ê°€"""

        # ì •ê·œí™” í™•ì¸
        if np.linalg.norm(vector) - 1.0 > 1e-6:
            vector = vector / np.linalg.norm(vector)

        # ì¸ë±ìŠ¤ì— ì¶”ê°€
        self.index.add(vector.reshape(1, -1))

        # ë§¤í•‘ ì €ì¥
        self.id_to_idx[node_id] = self.next_idx
        self.idx_to_id[self.next_idx] = node_id

        self.next_idx += 1

    def batch_add(self, node_ids: List[str], vectors: np.ndarray):
        """ë°°ì¹˜ ì¶”ê°€"""

        # ì •ê·œí™”
        norms = np.linalg.norm(vectors, axis=1, keepdims=True)
        vectors = vectors / norms

        # ì¸ë±ìŠ¤ì— ì¶”ê°€
        self.index.add(vectors)

        # ë§¤í•‘ ì €ì¥
        for node_id in node_ids:
            self.id_to_idx[node_id] = self.next_idx
            self.idx_to_id[self.next_idx] = node_id
            self.next_idx += 1

    def search(self, query_vector: np.ndarray, k: int = 10) -> List[Tuple[str, float]]:
        """ê°€ì¥ ìœ ì‚¬í•œ kê°œ ë…¸ë“œ ê²€ìƒ‰"""

        # ì •ê·œí™”
        query_vector = query_vector / np.linalg.norm(query_vector)

        # ê²€ìƒ‰
        distances, indices = self.index.search(query_vector.reshape(1, -1), k)

        # ê²°ê³¼ ë³€í™˜
        results = []
        for idx, distance in zip(indices[0], distances[0]):
            if idx in self.idx_to_id:
                node_id = self.idx_to_id[idx]
                similarity = float(distance)  # Inner Product = ì½”ì‚¬ì¸ ìœ ì‚¬ë„
                results.append((node_id, similarity))

        return results

    def save(self, filepath: str):
        """ì¸ë±ìŠ¤ ì €ì¥"""
        faiss.write_index(self.index, filepath)

        # ë§¤í•‘ ì €ì¥
        import pickle
        with open(filepath + '.mapping', 'wb') as f:
            pickle.dump((self.id_to_idx, self.idx_to_id, self.next_idx), f)

        print(f"ğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {filepath}")

    def load(self, filepath: str):
        """ì¸ë±ìŠ¤ ë¡œë“œ"""
        self.index = faiss.read_index(filepath)

        # ë§¤í•‘ ë¡œë“œ
        import pickle
        with open(filepath + '.mapping', 'rb') as f:
            self.id_to_idx, self.idx_to_id, self.next_idx = pickle.load(f)

        print(f"ğŸ“‚ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {self.next_idx}ê°œ ë²¡í„°")
```

---

## 2.4 NAS ì €ì¥ì†Œ ì„¤ê³„

### í´ë” êµ¬ì¡°

```
/Volumes/work-sync/project/ideamemo/
â”œâ”€â”€ nodes/                          # ë…¸ë“œ ë°ì´í„°
â”‚   â”œâ”€â”€ famous-quote/
â”‚   â”‚   â”œâ”€â”€ en/
â”‚   â”‚   â”‚   â”œâ”€â”€ batch_0001.json     # 1,000ê°œì”©
â”‚   â”‚   â”‚   â”œâ”€â”€ batch_0002.json
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â””â”€â”€ ko/
â”‚   â”‚       â”œâ”€â”€ batch_0001.json
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”œâ”€â”€ book/
â”‚   â”‚   â”œâ”€â”€ classic/
â”‚   â”‚   â”œâ”€â”€ self-help/
â”‚   â”‚   â””â”€â”€ philosophy/
â”‚   â”œâ”€â”€ movie/
â”‚   â”œâ”€â”€ proverb/
â”‚   â”œâ”€â”€ academic/
â”‚   â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ essay/
â”‚   â”œâ”€â”€ poem/
â”‚   â”œâ”€â”€ drama/
â”‚   â””â”€â”€ animation/
â”‚
â”œâ”€â”€ edges/                          # ì—£ì§€ ë°ì´í„°
â”‚   â”œâ”€â”€ semantic/                   # ì˜ë¯¸ì  ê´€ê³„
â”‚   â”œâ”€â”€ emotional/                  # ê°ì •ì  ê´€ê³„
â”‚   â”œâ”€â”€ causal/                     # ì¸ê³¼ ê´€ê³„
â”‚   â””â”€â”€ metaphorical/               # ì€ìœ ì  ê´€ê³„
â”‚
â”œâ”€â”€ embeddings/                     # ì„ë² ë”© ë²¡í„°
â”‚   â”œâ”€â”€ semantic/
â”‚   â”‚   â”œâ”€â”€ vectors.faiss           # FAISS ì¸ë±ìŠ¤
â”‚   â”‚   â””â”€â”€ vectors.faiss.mapping   # ID ë§¤í•‘
â”‚   â”œâ”€â”€ emotional/
â”‚   â””â”€â”€ multilingual/
â”‚
â”œâ”€â”€ indexes/                        # ë©”íƒ€ ì¸ë±ìŠ¤
â”‚   â”œâ”€â”€ master_index.json           # ì „ì²´ ë…¸ë“œ ëª©ë¡
â”‚   â”œâ”€â”€ category_index.json         # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
â”‚   â”œâ”€â”€ keyword_index.json          # í‚¤ì›Œë“œ ì—­ìƒ‰ì¸
â”‚   â”œâ”€â”€ frame_index.json            # í”„ë ˆì„ë³„ ë…¸ë“œ
â”‚   â””â”€â”€ metaphor_index.json         # ì€ìœ ë³„ ë…¸ë“œ
â”‚
â”œâ”€â”€ quality/                        # í’ˆì§ˆ ê´€ë¦¬
â”‚   â”œâ”€â”€ validation_reports/
â”‚   â”œâ”€â”€ curated_nodes.json          # ì „ë¬¸ê°€ ê²€ì¦ ì™„ë£Œ
â”‚   â””â”€â”€ flagged_nodes.json          # ë¬¸ì œ ìˆëŠ” ë…¸ë“œ
â”‚
â””â”€â”€ versions/                       # ë²„ì „ ê´€ë¦¬
    â”œâ”€â”€ v1.0/
    â”œâ”€â”€ v1.1/
    â””â”€â”€ v2.0/
```

### ë°ì´í„° íŒŒì¼ í¬ë§·

#### nodes/famous-quote/en/batch_0001.json
```json
{
  "batch_info": {
    "version": "2.0",
    "created_at": "2025-11-10T12:00:00Z",
    "node_count": 1000,
    "category": "famous-quote",
    "language": "en"
  },
  "nodes": [
    {
      "id": "fq_en_001",
      "content": "We are what we repeatedly do. Excellence, then, is not an act, but a habit.",
      "source": {
        "author": "Aristotle",
        "year": -350,
        "verified": true
      },
      "linguistic": {
        "primary_frame": "identity_formation",
        "metaphors": [
          {
            "system": "identity_is_construction",
            "strength": 0.85
          }
        ]
      },
      "embeddings": {
        "semantic": [...],
        "emotional": [...],
        "pragmatic": [...]
      }
    },
    ...
  ]
}
```

#### edges/semantic/batch_0001.json
```json
{
  "batch_info": {
    "version": "2.0",
    "created_at": "2025-11-10T14:00:00Z",
    "edge_count": 5000,
    "relation_type": "semantic"
  },
  "edges": [
    {
      "id": "edge_001",
      "from": "fq_en_001",
      "to": "fq_en_042",
      "relation_type": "similar_to",
      "strength": 0.87,
      "dimensions": {
        "semantic_similarity": 0.92,
        "emotional_resonance": 0.75
      },
      "reasoning": {
        "automatic": "Both quotes discuss habit formation and identity",
        "evidence": ["shared frame: habit_formation", "similar keywords"]
      }
    },
    ...
  ]
}
```

#### indexes/master_index.json
```json
{
  "version": "2.0",
  "updated_at": "2025-11-10T20:00:00Z",
  "stats": {
    "total_nodes": 52000,
    "total_edges": 548000,
    "categories": {
      "famous-quote": 10000,
      "book": 8000,
      "movie": 7000,
      "academic": 6000,
      "proverb": 5000,
      "web": 5000,
      "essay": 4000,
      "poem": 3000,
      "drama": 2000,
      "animation": 2000
    }
  },
  "files": [
    {
      "path": "nodes/famous-quote/en/batch_0001.json",
      "node_count": 1000,
      "size_mb": 15.3,
      "checksum": "sha256:abc123..."
    },
    ...
  ]
}
```

---

# Part 3: ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰ ê³„íš

## 3.1 50,000ê°œ ë…¸ë“œ ìˆ˜ì§‘ ì „ëµ

### 3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸

**Phase 1: Raw Collection (ì›ì‹œ ìˆ˜ì§‘)** - Week 1-4
- ëª©í‘œ: 50,000ê°œ ì›ì‹œ ë°ì´í„°
- ë°©ë²•: API + ì›¹ ìŠ¤í¬ë˜í•‘
- í’ˆì§ˆ: ê¸°ë³¸ ê²€ì¦ë§Œ

**Phase 2: Enrichment (ê°•í™”)** - Week 5-8
- ëª©í‘œ: ì–¸ì–´í•™Â·ì‹¬ë¦¬í•™ì  ì£¼ì„ ì¶”ê°€
- ë°©ë²•: NLP íŒŒì´í”„ë¼ì¸ ìë™í™”
- í’ˆì§ˆ: ìë™ í’ˆì§ˆ ì ìˆ˜

**Phase 3: Graph Construction (ê·¸ë˜í”„ êµ¬ì¶•)** - Week 9-12
- ëª©í‘œ: 500,000+ ì—£ì§€ ìƒì„±
- ë°©ë²•: ë‹¤ì°¨ì› ìœ ì‚¬ë„ ê³„ì‚°
- í’ˆì§ˆ: 3-Tier ê²€ì¦

---

## 3.2 ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ ê¸°ì¤€ (ìš”ì•½)

| ì¹´í…Œê³ ë¦¬ | ëª©í‘œ | ì£¼ìš” ì†ŒìŠ¤ | ìš°ì„ ìˆœìœ„ |
|---------|------|----------|---------|
| famous-quote | 10,000 | Quotable API, Wikiquote | â­â­â­ |
| book | 8,000 | Google Books API, Gutenberg | â­â­â­ |
| movie | 7,000 | IMDb, ì˜í™” ëŒ€ë³¸ DB | â­â­â­ |
| academic | 6,000 | arXiv, Google Scholar | â­â­ |
| proverb | 5,000 | ì†ë‹´ ì‚¬ì „, ì‚¬ìì„±ì–´ DB | â­â­ |
| web | 5,000 | Medium, ë¸ŒëŸ°ì¹˜ | â­â­ |
| essay | 4,000 | ìœ ëª… ì—ì„¸ì´ìŠ¤íŠ¸ ì‘í’ˆ | â­ |
| poem | 3,000 | ê³µê°œ ë„ë©”ì¸ ì‹œì§‘ | â­ |
| drama | 2,000 | ë“œë¼ë§ˆ ëª…ëŒ€ì‚¬ | â­ |
| animation | 2,000 | ì• ë‹ˆë©”ì´ì…˜ ëª…ëŒ€ì‚¬ | â­ |

---

## 3.3 ìë™ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸

### ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ êµ¬ì¡°

```python
# scripts/collection/collector_base.py

from abc import ABC, abstractmethod
from typing import List, Dict
import asyncio

class BaseCollector(ABC):
    """ê¸°ë³¸ ìˆ˜ì§‘ê¸° ì¶”ìƒ í´ë˜ìŠ¤"""

    def __init__(self, category: str, target_count: int):
        self.category = category
        self.target_count = target_count
        self.collected = []

    @abstractmethod
    async def fetch_batch(self, batch_size: int = 100) -> List[Dict]:
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ìˆ˜ì§‘"""
        pass

    @abstractmethod
    def validate(self, item: Dict) -> bool:
        """ë°ì´í„° ê²€ì¦"""
        pass

    def transform(self, raw_item: Dict) -> Dict:
        """ì›ì‹œ ë°ì´í„°ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        return {
            'id': self.generate_id(),
            'content': raw_item['text'],
            'source': {
                'author': raw_item.get('author', 'Unknown'),
                'verified': False
            },
            'classification': {
                'primary_category': self.category,
                'keywords': []
            }
        }

    async def collect_all(self):
        """ì „ì²´ ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤"""
        print(f"ğŸš€ {self.category} ìˆ˜ì§‘ ì‹œì‘ (ëª©í‘œ: {self.target_count})")

        while len(self.collected) < self.target_count:
            try:
                batch = await self.fetch_batch()

                for item in batch:
                    if self.validate(item):
                        transformed = self.transform(item)
                        self.collected.append(transformed)

                print(f"  âœ… {len(self.collected)} / {self.target_count}")

                # Rate limiting
                await asyncio.sleep(1)

            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(5)

        print(f"ğŸ‰ {self.category} ìˆ˜ì§‘ ì™„ë£Œ!")
        return self.collected
```

### Quotable API ìˆ˜ì§‘ê¸°

```python
# scripts/collection/quotable_collector.py

import aiohttp
from collector_base import BaseCollector

class QuotableCollector(BaseCollector):
    """Quotable.io API ìˆ˜ì§‘ê¸°"""

    def __init__(self):
        super().__init__('famous-quote', 10000)
        self.api_url = 'https://api.quotable.io'
        self.current_page = 1

    async def fetch_batch(self, batch_size: int = 100):
        """Quotable APIì—ì„œ ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°"""
        async with aiohttp.ClientSession() as session:
            url = f"{self.api_url}/quotes?page={self.current_page}&limit={batch_size}"

            async with session.get(url) as response:
                data = await response.json()
                self.current_page += 1

                return data.get('results', [])

    def validate(self, item: Dict) -> bool:
        """ê²€ì¦"""
        # ê¸¸ì´ ì²´í¬
        if len(item.get('content', '')) < 10:
            return False
        if len(item.get('content', '')) > 500:
            return False

        # ì €ì ì²´í¬
        if not item.get('author'):
            return False

        return True

    def transform(self, raw_item: Dict) -> Dict:
        """Quotable ë°ì´í„°ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ"""
        return {
            'id': f"fq_quotable_{raw_item['_id']}",
            'content': raw_item['content'],
            'source': {
                'author': raw_item['author'],
                'verified': True,
                'url': f"https://quotable.io/quotes/{raw_item['_id']}"
            },
            'classification': {
                'primary_category': 'famous-quote',
                'keywords': raw_item.get('tags', [])
            }
        }
```

---

## 3.4 í’ˆì§ˆ ê´€ë¦¬ (3-Tier ê²€ì¦)

### Tier 1: ìë™ ê²€ì¦ (100%)

```python
# scripts/quality/auto_validator.py

class AutoValidator:
    """ìë™ í’ˆì§ˆ ê²€ì¦"""

    def validate_node(self, node: Dict) -> Dict:
        """ë…¸ë“œ ê²€ì¦ ë° ì ìˆ˜ ì‚°ì •"""

        errors = []
        warnings = []
        score = 100

        # 1. í•„ìˆ˜ í•„ë“œ
        required_fields = ['id', 'content', 'source']
        for field in required_fields:
            if field not in node:
                errors.append(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
                score -= 30

        # 2. ê¸¸ì´ ê²€ì¦
        content_len = len(node.get('content', ''))
        if content_len < 10:
            errors.append("ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŒ")
            score -= 40
        elif content_len > 500:
            warnings.append("ë‚´ìš©ì´ ê¸¸ì–´ ì¸ì§€ ë¶€í•˜ ìš°ë ¤")
            score -= 10

        # 3. ì¤‘ë³µ ê²€ì‚¬
        if self.is_duplicate(node['content']):
            errors.append("ì¤‘ë³µëœ ë‚´ìš©")
            score -= 50

        # 4. ì–¸ì–´ ê°ì§€
        detected_lang = self.detect_language(node['content'])
        if detected_lang not in ['en', 'ko']:
            warnings.append(f"ë¹„í‘œì¤€ ì–¸ì–´: {detected_lang}")
            score -= 5

        return {
            'valid': len(errors) == 0,
            'score': max(0, score),
            'errors': errors,
            'warnings': warnings
        }

    def is_duplicate(self, content: str) -> bool:
        """ì¤‘ë³µ ì²´í¬ (ì²« 50ì fingerprint)"""
        fingerprint = content[:50].lower()
        fingerprint = ''.join(c for c in fingerprint if c.isalnum())

        if fingerprint in self.seen_fingerprints:
            return True

        self.seen_fingerprints.add(fingerprint)
        return False
```

### Tier 2: ì „ë¬¸ê°€ íë ˆì´ì…˜ (10%)

```python
# scripts/quality/expert_curation.py

class ExpertCuration:
    """ì „ë¬¸ê°€ íë ˆì´ì…˜ ì‹œìŠ¤í…œ"""

    def select_for_review(self, nodes: List[Dict]) -> List[Dict]:
        """ê²€í† ê°€ í•„ìš”í•œ ë…¸ë“œ ì„ ë³„"""

        candidates = []

        for node in nodes:
            # 1. ë‚®ì€ ìë™ ì ìˆ˜
            if node.get('quality', {}).get('auto_score', 100) < 70:
                candidates.append(('low_score', node))

            # 2. ë†’ì€ ì—°ê²°ì„± (í—ˆë¸Œ ë…¸ë“œ)
            if node.get('stats', {}).get('connection_count', 0) > 50:
                candidates.append(('hub', node))

            # 3. ë³µì¡í•œ ì–¸ì–´í•™ì  êµ¬ì¡°
            if len(node.get('linguistic', {}).get('metaphors', [])) > 2:
                candidates.append(('complex', node))

        # ìƒìœ„ 10% ì„ íƒ
        selected = candidates[:len(nodes) // 10]
        return selected

    def create_review_task(self, node: Dict, reason: str) -> Dict:
        """ê²€í†  ì‘ì—… ìƒì„±"""
        return {
            'node_id': node['id'],
            'reason': reason,
            'content': node['content'],
            'auto_analysis': {
                'frame': node.get('linguistic', {}).get('primary_frame'),
                'metaphors': node.get('linguistic', {}).get('metaphors'),
                'emotions': node.get('affective', {}).get('primary_emotion')
            },
            'questions': [
                "í”„ë ˆì„ ë¶„ë¥˜ê°€ ì •í™•í•œê°€?",
                "ì€ìœ  ê°ì§€ê°€ ì ì ˆí•œê°€?",
                "ê°ì • ë¶„ì„ì´ íƒ€ë‹¹í•œê°€?",
                "ë†“ì¹œ ê´€ê³„ê°€ ìˆëŠ”ê°€?"
            ],
            'status': 'pending'
        }
```

### Tier 3: ì»¤ë®¤ë‹ˆí‹° í”¼ë“œë°±

```python
# scripts/quality/community_feedback.py

class CommunityFeedback:
    """ì»¤ë®¤ë‹ˆí‹° í”¼ë“œë°± ìˆ˜ì§‘"""

    def collect_implicit_feedback(self, user_session: Dict) -> List[Dict]:
        """ì‚¬ìš©ì í–‰ë™ ê¸°ë°˜ ì•”ë¬µì  í”¼ë“œë°±"""

        feedback = []

        # 1. ì²´ë¥˜ ì‹œê°„ (10ì´ˆ ì´ìƒ = ê´€ì‹¬)
        for node_id, duration in user_session.get('dwell_times', {}).items():
            if duration > 10:
                feedback.append({
                    'type': 'positive_engagement',
                    'node_id': node_id,
                    'strength': min(duration / 60, 1.0),
                    'timestamp': user_session['timestamp']
                })

        # 2. ì €ì¥/ë¶ë§ˆí¬
        for node_id in user_session.get('saved_nodes', []):
            feedback.append({
                'type': 'explicit_save',
                'node_id': node_id,
                'strength': 1.0,
                'timestamp': user_session['timestamp']
            })

        # 3. ì‚¬ìš©ì ë©”ëª¨ì™€ ì—°ê²°
        for link in user_session.get('user_note_links', []):
            feedback.append({
                'type': 'user_integration',
                'node_id': link['idea_node'],
                'strength': 0.9,
                'timestamp': user_session['timestamp']
            })

        return feedback
```

---

## 3.5 ê·¸ë˜í”„ êµ¬ì¶• íŒŒì´í”„ë¼ì¸

```python
# scripts/graph/graph_builder.py

import networkx as nx
from typing import List, Dict
import numpy as np

class KnowledgeGraphBuilder:
    """ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶•ê¸°"""

    def __init__(self):
        self.graph = nx.DiGraph()
        self.embedding_system = MultiEmbeddingSystem()
        self.vector_index = VectorIndex()

    def build(self, nodes: List[Dict]) -> nx.DiGraph:
        """ë…¸ë“œ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ê·¸ë˜í”„ êµ¬ì¶•"""

        print("ğŸ”¨ ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• ì‹œì‘...")

        # 1. ë…¸ë“œ ì¶”ê°€
        print("  ğŸ“ ë…¸ë“œ ì¶”ê°€ ì¤‘...")
        for node in nodes:
            self.graph.add_node(node['id'], data=node)
        print(f"    âœ… {len(nodes)}ê°œ ë…¸ë“œ ì¶”ê°€ ì™„ë£Œ")

        # 2. ì„ë² ë”© ìƒì„±
        print("  ğŸ§¬ ì„ë² ë”© ìƒì„± ì¤‘...")
        embeddings = self.embedding_system.batch_encode(nodes)

        # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
        node_ids = [node['id'] for node in nodes]
        self.vector_index.batch_add(node_ids, embeddings['semantic'])
        print(f"    âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ")

        # 3. ì—£ì§€ ìë™ ìƒì„±
        print("  ğŸ”— ì—£ì§€ ìƒì„± ì¤‘...")
        edge_count = 0

        for i, node1 in enumerate(nodes):
            # ê° ë…¸ë“œë§ˆë‹¤ ê°€ì¥ ìœ ì‚¬í•œ kê°œ ë…¸ë“œ ì°¾ê¸°
            similar = self.vector_index.search(
                embeddings['semantic'][i],
                k=20  # ìƒìœ„ 20ê°œ
            )

            for node2_id, similarity in similar:
                if node2_id == node1['id']:
                    continue

                node2 = self.graph.nodes[node2_id]['data']

                # ë‹¤ì°¨ì› ìœ ì‚¬ë„ ê³„ì‚°
                edge = self.create_edge(node1, node2, similarity)

                if edge and edge['strength'] >= 0.65:
                    self.graph.add_edge(
                        node1['id'],
                        node2_id,
                        data=edge
                    )
                    edge_count += 1

            if (i + 1) % 1000 == 0:
                print(f"    ì§„í–‰: {i + 1}/{len(nodes)} ë…¸ë“œ ì²˜ë¦¬ ({edge_count}ê°œ ì—£ì§€)")

        print(f"    âœ… {edge_count}ê°œ ì—£ì§€ ìƒì„± ì™„ë£Œ")

        # 4. ê·¸ë˜í”„ ìµœì í™”
        print("  âš¡ ê·¸ë˜í”„ ìµœì í™” ì¤‘...")
        self.optimize_graph()
        print(f"    âœ… ìµœì í™” ì™„ë£Œ")

        print(f"ğŸ‰ ê·¸ë˜í”„ êµ¬ì¶• ì™„ë£Œ!")
        print(f"   ë…¸ë“œ: {self.graph.number_of_nodes()}")
        print(f"   ì—£ì§€: {self.graph.number_of_edges()}")
        print(f"   í‰ê·  ì—°ê²°ë„: {self.graph.number_of_edges() / self.graph.number_of_nodes():.2f}")

        return self.graph

    def optimize_graph(self):
        """ê·¸ë˜í”„ ìµœì í™”"""

        # 1. ì•½í•œ ì—£ì§€ ì œê±° (strength < 0.5)
        weak_edges = [
            (u, v) for u, v, data in self.graph.edges(data=True)
            if data['data']['strength'] < 0.5
        ]
        self.graph.remove_edges_from(weak_edges)
        print(f"      - {len(weak_edges)}ê°œ ì•½í•œ ì—£ì§€ ì œê±°")

        # 2. ì»¤ë®¤ë‹ˆí‹° ê°ì§€
        from networkx.algorithms import community
        communities = community.greedy_modularity_communities(self.graph.to_undirected())
        print(f"      - {len(communities)}ê°œ ì»¤ë®¤ë‹ˆí‹° ê°ì§€")

        # 3. ì¤‘ì‹¬ì„± ê³„ì‚°
        pagerank = nx.pagerank(self.graph)

        # ë…¸ë“œì— ì¤‘ì‹¬ì„± ì ìˆ˜ ì¶”ê°€
        for node_id, score in pagerank.items():
            self.graph.nodes[node_id]['data']['centrality'] = score
```

---

# Part 4: UI/UX ê°œì„  (MZì„¸ëŒ€ ë§ì¶¤)

## 4.1 í˜„ì¬ UI ë¬¸ì œì 

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Header                       â”‚ â† í‰ë²”
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Sidebar  â”‚ Main             â”‚
â”‚          â”‚                  â”‚
â”‚ ë©”ëª¨     â”‚ ì—ë””í„°           â”‚ â† ê¸°ëŠ¥ì ì´ì§€ë§Œ ë°‹ë°‹
â”‚ ëª©ë¡     â”‚                  â”‚
â”‚          â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ë¬¸ì œ:**
- âŒ TailwindCSS ê¸°ë³¸ ìŠ¤íƒ€ì¼ (ë‹¤ë¥¸ ì•±ê³¼ êµ¬ë³„ ì•ˆ ë¨)
- âŒ ê°ì„± ì œë¡œ
- âŒ SNS ê³µìœ í•˜ê³  ì‹¶ì§€ ì•ŠìŒ
- âŒ ë§ˆì´í¬ë¡œ ì¸í„°ë™ì…˜ ë¶€ì¬

---

## 4.2 6ê°€ì§€ ê°ì„± í…Œë§ˆ

```typescript
// src/themes/index.ts

export const themes = {
  // 1. ë¯¸ë‹ˆë©€ í™”ì´íŠ¸ (ê¸°ë³¸)
  minimal: {
    name: 'ë¯¸ë‹ˆë©€ í™”ì´íŠ¸',
    colors: {
      primary: '#000000',
      background: '#FFFFFF',
      surface: '#F8F9FA',
      text: '#1A1A1A',
      accent: '#4A90E2'
    },
    fonts: {
      body: 'Pretendard Variable',
      heading: 'Pretendard Variable',
      quote: 'Georgia'
    }
  },

  // 2. ë‹¤í¬ ëª¨ë“œ (AMOLED Black)
  dark: {
    name: 'ë‹¤í¬ ëª¨ë“œ',
    colors: {
      primary: '#FFFFFF',
      background: '#000000',  // Pure black for AMOLED
      surface: '#1A1A1A',
      text: '#E5E5E5',
      accent: '#7C3AED'
    }
  },

  // 3. ë”°ëœ»í•œ ë² ì´ì§€ (ì¹´í˜ ê°ì„±)
  warm: {
    name: 'ë”°ëœ»í•œ ë² ì´ì§€',
    colors: {
      primary: '#3E2723',
      background: '#FFF8E1',
      surface: '#FFECB3',
      text: '#4E342E',
      accent: '#FF6F00'
    }
  },

  // 4. íŒŒìŠ¤í…” ë¸”ë£¨ (ì°¨ë¶„í•¨)
  pastel: {
    name: 'íŒŒìŠ¤í…” ë¸”ë£¨',
    colors: {
      primary: '#1565C0',
      background: '#E3F2FD',
      surface: '#BBDEFB',
      text: '#0D47A1',
      accent: '#42A5F5'
    }
  },

  // 5. ì„ ì…‹ ê·¸ë¼ë””ì–¸íŠ¸ (ê°ì„±)
  sunset: {
    name: 'ì„ ì…‹ ê·¸ë¼ë””ì–¸íŠ¸',
    colors: {
      primary: '#FF6B6B',
      background: 'linear-gradient(135deg, #667eea 0%, #764ba2 100%)',
      surface: 'rgba(255, 255, 255, 0.1)',
      text: '#FFFFFF',
      accent: '#FFA07A'
    }
  },

  // 6. í¬ë ˆìŠ¤íŠ¸ ê·¸ë¦° (ìì—°)
  forest: {
    name: 'í¬ë ˆìŠ¤íŠ¸ ê·¸ë¦°',
    colors: {
      primary: '#2E7D32',
      background: '#E8F5E9',
      surface: '#C8E6C9',
      text: '#1B5E20',
      accent: '#66BB6A'
    }
  }
};
```

---

## 4.3 ë§ˆì´í¬ë¡œ ì¸í„°ë™ì…˜

```typescript
// src/components/MicroInteractions.tsx

import { motion, AnimatePresence } from 'framer-motion';

// 1. ë©”ëª¨ ì €ì¥ ì‹œ ë°˜ì§ì„ íš¨ê³¼
export function SparkleEffect({ x, y }: { x: number; y: number }) {
  return (
    <motion.div
      className="absolute pointer-events-none"
      style={{ left: x, top: y }}
      initial={{ scale: 0, opacity: 1 }}
      animate={{ scale: 1.5, opacity: 0 }}
      exit={{ opacity: 0 }}
      transition={{ duration: 0.6 }}
    >
      âœ¨
    </motion.div>
  );
}

// 2. ì•„ì´ë””ì–´ ì—°ê²° ì‹œ ì²´ì¸ ì• ë‹ˆë©”ì´ì…˜
export function ConnectionChain({ from, to }: { from: Element; to: Element }) {
  const fromRect = from.getBoundingClientRect();
  const toRect = to.getBoundingClientRect();

  return (
    <svg className="absolute inset-0 pointer-events-none">
      <motion.path
        d={`M ${fromRect.left} ${fromRect.top} Q ${(fromRect.left + toRect.left) / 2} ${(fromRect.top + toRect.top) / 2 - 50} ${toRect.left} ${toRect.top}`}
        stroke="#4A90E2"
        strokeWidth={2}
        fill="none"
        initial={{ pathLength: 0 }}
        animate={{ pathLength: 1 }}
        transition={{ duration: 0.8, ease: "easeInOut" }}
      />
    </svg>
  );
}

// 3. ì¢‹ì•„ìš” í´ë¦­ ì‹œ í•˜íŠ¸ í„°ì§€ê¸°
export function HeartBurst({ x, y }: { x: number; y: number }) {
  const hearts = Array.from({ length: 8 }, (_, i) => ({
    angle: (i * 360) / 8,
    delay: i * 0.05
  }));

  return (
    <div className="absolute pointer-events-none" style={{ left: x, top: y }}>
      {hearts.map(({ angle, delay }, i) => (
        <motion.div
          key={i}
          className="absolute"
          initial={{ x: 0, y: 0, opacity: 1, scale: 0 }}
          animate={{
            x: Math.cos((angle * Math.PI) / 180) * 50,
            y: Math.sin((angle * Math.PI) / 180) * 50,
            opacity: 0,
            scale: 1.5
          }}
          transition={{ duration: 0.8, delay }}
        >
          â¤ï¸
        </motion.div>
      ))}
    </div>
  );
}

// 4. ê·¸ë˜í”„ ë…¸ë“œ í˜¸ë²„ íš¨ê³¼
export function GraphNode({ node, onHover }: { node: any; onHover: () => void }) {
  return (
    <motion.div
      className="graph-node"
      whileHover={{
        scale: 1.2,
        boxShadow: '0 0 20px rgba(74, 144, 226, 0.5)'
      }}
      onHoverStart={onHover}
      transition={{ type: "spring", stiffness: 300 }}
    >
      {node.content.substring(0, 50)}...
    </motion.div>
  );
}
```

---

## 4.4 ì´ë¯¸ì§€ ê³µìœ  ê¸°ëŠ¥

```typescript
// src/features/share/MemoToImage.tsx

import html2canvas from 'html2canvas';

export function MemoToImage({ memo, idea }: { memo: Memo; idea: Idea }) {
  const canvasRef = useRef<HTMLDivElement>(null);

  const generateImage = async () => {
    if (!canvasRef.current) return;

    const canvas = await html2canvas(canvasRef.current, {
      width: 1080,
      height: 1080,
      scale: 2
    });

    // PNGë¡œ ë³€í™˜
    const dataUrl = canvas.toDataURL('image/png');

    // ë‹¤ìš´ë¡œë“œ
    const link = document.createElement('a');
    link.download = `ideaconnect-${memo.id}.png`;
    link.href = dataUrl;
    link.click();
  };

  return (
    <div>
      {/* ì‹¤ì œ ë Œë”ë§ë  ì´ë¯¸ì§€ (hidden) */}
      <div
        ref={canvasRef}
        className="hidden"
        style={{
          width: '1080px',
          height: '1080px',
          background: 'linear-gradient(135deg, #667eea 0%, #764ba2 100%)',
          padding: '60px',
          display: 'flex',
          flexDirection: 'column',
          justifyContent: 'space-between'
        }}
      >
        {/* ë©”ëª¨ ë‚´ìš© */}
        <div style={{ background: 'white', borderRadius: '24px', padding: '48px' }}>
          <h1 style={{ fontSize: '48px', fontWeight: 'bold', marginBottom: '24px' }}>
            {memo.title}
          </h1>
          <p style={{ fontSize: '24px', color: '#555', lineHeight: '1.6' }}>
            {memo.content.substring(0, 200)}...
          </p>
        </div>

        {/* ì—°ê²°ëœ ì•„ì´ë””ì–´ */}
        <div style={{
          borderTop: '4px solid rgba(255,255,255,0.3)',
          paddingTop: '24px',
          color: 'white'
        }}>
          <p style={{ fontSize: '32px', fontStyle: 'italic', marginBottom: '16px' }}>
            "{idea.content}"
          </p>
          <p style={{ textAlign: 'right', fontSize: '20px', opacity: 0.8 }}>
            â€” {idea.source.author}
          </p>
        </div>

        {/* ë¡œê³  */}
        <div style={{ textAlign: 'center', color: 'white', fontSize: '18px', opacity: 0.7 }}>
          ğŸ’¡ IdeaConnect
        </div>
      </div>

      {/* ìƒì„± ë²„íŠ¼ */}
      <Button onClick={generateImage}>
        ğŸ“¸ ì´ë¯¸ì§€ë¡œ ì €ì¥
      </Button>
    </div>
  );
}
```

---

## 4.5 ê·¸ë˜í”„ ì‹œê°í™” (ì˜µì‹œë””ì–¸ ìŠ¤íƒ€ì¼)

```typescript
// src/features/graph/GraphView3D.tsx

import { useRef, useEffect } from 'react';
import ForceGraph3D from 'react-force-graph-3d';

export function GraphView3D({ nodes, edges }: { nodes: any[]; edges: any[] }) {
  const fgRef = useRef<any>();

  const graphData = {
    nodes: nodes.map(node => ({
      id: node.id,
      name: node.content.substring(0, 50),
      val: node.stats.connection_count,
      color: getNodeColor(node.affective.primary_emotion)
    })),
    links: edges.map(edge => ({
      source: edge.from,
      target: edge.to,
      value: edge.strength,
      color: getEdgeColor(edge.relation_type)
    }))
  };

  useEffect(() => {
    // ì¹´ë©”ë¼ ì• ë‹ˆë©”ì´ì…˜
    const fg = fgRef.current;
    if (fg) {
      fg.cameraPosition({ z: 300 }, null, 2000);
    }
  }, []);

  return (
    <div className="h-screen w-full">
      <ForceGraph3D
        ref={fgRef}
        graphData={graphData}
        nodeLabel="name"
        nodeAutoColorBy="group"
        linkDirectionalParticles={2}
        linkDirectionalParticleSpeed={0.005}
        onNodeClick={handleNodeClick}
        onNodeHover={handleNodeHover}
        nodeThreeObject={(node: any) => {
          // ì»¤ìŠ¤í…€ 3D ì˜¤ë¸Œì íŠ¸
          const sprite = new SpriteText(node.name);
          sprite.color = node.color;
          sprite.textHeight = 8;
          return sprite;
        }}
      />
    </div>
  );
}

function getNodeColor(emotion: string): string {
  const emotionColors = {
    joy: '#FFD700',
    trust: '#87CEEB',
    fear: '#9370DB',
    surprise: '#FF69B4',
    sadness: '#4682B4',
    anger: '#DC143C',
    anticipation: '#FFA500'
  };
  return emotionColors[emotion] || '#808080';
}
```

---

# Part 5: ê¸°ìˆ  ë¶€ì±„ & ê°œì„ 

## 5.1 í˜„ì¬ ê¸°ìˆ  ìŠ¤íƒ í‰ê°€

### âœ… ì˜í•œ ì„ íƒ

| ê¸°ìˆ  | ì´ìœ  | í‰ê°€ |
|------|------|------|
| React + TypeScript | ì—…ê³„ í‘œì¤€, íƒ€ì… ì•ˆì „ | â­â­â­â­â­ |
| Vite | ë¹ ë¥¸ ë¹Œë“œ, HMR | â­â­â­â­â­ |
| TailwindCSS | ìƒì‚°ì„±, ì¼ê´€ì„± | â­â­â­â­â­ |
| Zustand | ê°€ë³ê³  ê°„ë‹¨í•œ ìƒíƒœ ê´€ë¦¬ | â­â­â­â­â­ |
| IndexedDB (Dexie) | ì˜¤í”„ë¼ì¸ ìš°ì„ , ëŒ€ìš©ëŸ‰ | â­â­â­â­â­ |

### âš ï¸ ê°œì„  í•„ìš”

| í•­ëª© | í˜„ì¬ ìƒíƒœ | ë¬¸ì œ | í•´ê²° ë°©ì•ˆ |
|------|---------|------|----------|
| **í…ŒìŠ¤íŠ¸** | 0ê°œ | ë²„ê·¸ ìœ„í—˜ ë†’ìŒ | Vitest + Testing Library |
| **ì—ëŸ¬ ì²˜ë¦¬** | ê¸°ë³¸ë§Œ | UX ë¶ˆì¹œì ˆ | Error Boundary + Sentry |
| **ì„±ëŠ¥ ì¸¡ì •** | ì—†ìŒ | ìµœì í™” ì–´ë ¤ì›€ | Lighthouse CI |
| **ì ‘ê·¼ì„±** | ë¯¸í¡ | ì¥ì• ì¸ ì‚¬ìš© ë¶ˆê°€ | ARIA + í‚¤ë³´ë“œ ì§€ì› |

---

## 5.2 í…ŒìŠ¤íŠ¸ ì „ëµ

```typescript
// tests/unit/ideaStore.test.ts

import { describe, it, expect, beforeEach } from 'vitest';
import { useIdeaStore } from '@/stores/ideaStore';

describe('IdeaStore', () => {
  beforeEach(() => {
    // ê° í…ŒìŠ¤íŠ¸ ì „ ì´ˆê¸°í™”
    useIdeaStore.getState().reset();
  });

  it('should add a new idea', async () => {
    const store = useIdeaStore.getState();

    const idea = {
      content: 'Test quote',
      source: { author: 'Test Author' },
      type: 'famous-quote' as const
    };

    await store.addIdea(idea);

    const ideas = store.ideas;
    expect(ideas).toHaveLength(1);
    expect(ideas[0].content).toBe('Test quote');
  });

  it('should update feedback correctly', async () => {
    const store = useIdeaStore.getState();

    // ì•„ì´ë””ì–´ ì¶”ê°€
    await store.addIdea({
      content: 'Test',
      source: { author: 'Author' },
      type: 'famous-quote'
    });

    const ideaId = store.ideas[0].id;

    // í”¼ë“œë°± ì—…ë°ì´íŠ¸
    await store.updateFeedback(ideaId, 'up');

    const idea = store.ideas.find(i => i.id === ideaId);
    expect(idea?.feedback).toBe('up');
  });
});
```

---

## 5.3 ì—ëŸ¬ ì²˜ë¦¬ & ëª¨ë‹ˆí„°ë§

```typescript
// src/components/ErrorBoundary.tsx

import { Component, ReactNode } from 'react';
import * as Sentry from '@sentry/react';

interface Props {
  children: ReactNode;
  fallback?: ReactNode;
}

interface State {
  hasError: boolean;
  error?: Error;
}

export class ErrorBoundary extends Component<Props, State> {
  constructor(props: Props) {
    super(props);
    this.state = { hasError: false };
  }

  static getDerivedStateFromError(error: Error): State {
    return { hasError: true, error };
  }

  componentDidCatch(error: Error, errorInfo: any) {
    // Sentryì— ì—ëŸ¬ ë³´ê³ 
    Sentry.captureException(error, { extra: errorInfo });

    console.error('Caught error:', error, errorInfo);
  }

  render() {
    if (this.state.hasError) {
      return (
        this.props.fallback || (
          <div className="min-h-screen flex items-center justify-center bg-gray-50">
            <div className="max-w-md p-8 bg-white rounded-lg shadow-lg">
              <h2 className="text-2xl font-bold text-red-600 mb-4">
                âš ï¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤
              </h2>
              <p className="text-gray-700 mb-4">
                ì£„ì†¡í•©ë‹ˆë‹¤. ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.
              </p>
              <p className="text-sm text-gray-500 mb-6">
                {this.state.error?.message}
              </p>
              <button
                onClick={() => window.location.reload()}
                className="px-4 py-2 bg-blue-500 text-white rounded hover:bg-blue-600"
              >
                í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
              </button>
            </div>
          </div>
        )
      );
    }

    return this.props.children;
  }
}

// Sentry ì´ˆê¸°í™”
Sentry.init({
  dsn: import.meta.env.VITE_SENTRY_DSN,
  environment: import.meta.env.MODE,
  tracesSampleRate: 1.0,
  integrations: [
    new Sentry.BrowserTracing(),
    new Sentry.Replay()
  ]
});
```

---

## 5.4 ì„±ëŠ¥ ìµœì í™”

```typescript
// src/hooks/useVirtualScroll.ts

import { useEffect, useRef, useState } from 'react';

export function useVirtualScroll<T>(
  items: T[],
  itemHeight: number,
  containerHeight: number
) {
  const [scrollTop, setScrollTop] = useState(0);
  const containerRef = useRef<HTMLDivElement>(null);

  // ë³´ì´ëŠ” í•­ëª©ë§Œ ë Œë”ë§
  const startIndex = Math.floor(scrollTop / itemHeight);
  const endIndex = Math.min(
    startIndex + Math.ceil(containerHeight / itemHeight) + 1,
    items.length
  );

  const visibleItems = items.slice(startIndex, endIndex);
  const offsetY = startIndex * itemHeight;

  useEffect(() => {
    const container = containerRef.current;
    if (!container) return;

    const handleScroll = () => {
      setScrollTop(container.scrollTop);
    };

    container.addEventListener('scroll', handleScroll);
    return () => container.removeEventListener('scroll', handleScroll);
  }, []);

  return {
    containerRef,
    visibleItems,
    offsetY,
    totalHeight: items.length * itemHeight
  };
}

// ì‚¬ìš© ì˜ˆì‹œ
function MemoList({ memos }: { memos: Memo[] }) {
  const { containerRef, visibleItems, offsetY, totalHeight } = useVirtualScroll(
    memos,
    80,  // ê° í•­ëª© ë†’ì´
    window.innerHeight
  );

  return (
    <div ref={containerRef} className="h-screen overflow-auto">
      <div style={{ height: totalHeight, position: 'relative' }}>
        <div style={{ transform: `translateY(${offsetY}px)` }}>
          {visibleItems.map(memo => (
            <MemoItem key={memo.id} memo={memo} />
          ))}
        </div>
      </div>
    </div>
  );
}
```

---

## 5.5 ì ‘ê·¼ì„± (a11y)

```typescript
// src/components/AccessibleButton.tsx

interface Props {
  onClick: () => void;
  children: React.ReactNode;
  ariaLabel: string;
  variant?: 'primary' | 'secondary';
}

export function AccessibleButton({ onClick, children, ariaLabel, variant = 'primary' }: Props) {
  return (
    <button
      onClick={onClick}
      aria-label={ariaLabel}
      className={`
        px-4 py-2 rounded
        focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-offset-2
        ${variant === 'primary' ? 'bg-blue-500 text-white' : 'bg-gray-200 text-gray-800'}
        hover:opacity-90
        transition-opacity
      `}
      // í‚¤ë³´ë“œ ë„¤ë¹„ê²Œì´ì…˜
      tabIndex={0}
      onKeyDown={(e) => {
        if (e.key === 'Enter' || e.key === ' ') {
          e.preventDefault();
          onClick();
        }
      }}
    >
      {children}
    </button>
  );
}

// Skip to content ë§í¬
export function SkipToContent() {
  return (
    <a
      href="#main-content"
      className="sr-only focus:not-sr-only focus:absolute focus:top-0 focus:left-0 bg-blue-500 text-white p-4 z-50"
    >
      ë³¸ë¬¸ìœ¼ë¡œ ê±´ë„ˆë›°ê¸°
    </a>
  );
}
```

---

# Part 6: í™•ì¥ì„± ì„¤ê³„

## 6.1 í”ŒëŸ¬ê·¸ì¸ ì‹œìŠ¤í…œ

```typescript
// src/plugins/types.ts

export interface Plugin {
  id: string;
  name: string;
  version: string;
  author: string;

  // í›…
  onMemoCreated?: (memo: Memo) => void | Promise<void>;
  onIdeaAdded?: (idea: Idea) => void | Promise<void>;
  onConnectionCreated?: (connection: Connection) => void | Promise<void>;

  // UI í™•ì¥
  sidebarWidget?: React.ComponentType;
  editorButton?: React.ComponentType<{ memo: Memo }>;
  settingsPanel?: React.ComponentType;

  // API
  api?: {
    search?: (query: string) => Promise<Idea[]>;
    export?: (data: any) => Promise<void>;
    import?: (file: File) => Promise<any>;
  };

  // ì„¤ì •
  settings?: Record<string, any>;

  // ë¼ì´í”„ì‚¬ì´í´
  onActivate?: () => void | Promise<void>;
  onDeactivate?: () => void | Promise<void>;
}

// í”ŒëŸ¬ê·¸ì¸ ë§¤ë‹ˆì €
export class PluginManager {
  private plugins: Map<string, Plugin> = new Map();

  register(plugin: Plugin) {
    this.plugins.set(plugin.id, plugin);
    plugin.onActivate?.();
    console.log(`âœ… í”ŒëŸ¬ê·¸ì¸ ë“±ë¡: ${plugin.name}`);
  }

  unregister(pluginId: string) {
    const plugin = this.plugins.get(pluginId);
    if (plugin) {
      plugin.onDeactivate?.();
      this.plugins.delete(pluginId);
      console.log(`âŒ í”ŒëŸ¬ê·¸ì¸ ì œê±°: ${plugin.name}`);
    }
  }

  // í›… ì‹¤í–‰
  async triggerHook(hookName: string, ...args: any[]) {
    for (const plugin of this.plugins.values()) {
      const hook = plugin[hookName as keyof Plugin];
      if (typeof hook === 'function') {
        await hook(...args);
      }
    }
  }
}
```

### í”ŒëŸ¬ê·¸ì¸ ì˜ˆì‹œ: ë§ì¶¤ë²• ê²€ì‚¬ê¸°

```typescript
// plugins/grammar-checker/index.ts

const grammarPlugin: Plugin = {
  id: 'grammar-checker',
  name: 'ë§ì¶¤ë²• ê²€ì‚¬ê¸°',
  version: '1.0.0',
  author: 'IdeaConnect Team',

  onMemoCreated: async (memo) => {
    // ë§ì¶¤ë²• ê²€ì‚¬
    const errors = await checkGrammar(memo.content);

    if (errors.length > 0) {
      // ì•Œë¦¼ í‘œì‹œ
      showNotification({
        title: 'ë§ì¶¤ë²• ì˜¤ë¥˜ ë°œê²¬',
        message: `${errors.length}ê°œì˜ ì˜¤ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤.`,
        type: 'warning'
      });
    }
  },

  editorButton: ({ memo }) => (
    <button onClick={() => correctGrammar(memo)}>
      âœ“ ë§ì¶¤ë²• ê²€ì‚¬
    </button>
  ),

  settingsPanel: () => (
    <div>
      <h3>ë§ì¶¤ë²• ê²€ì‚¬ ì„¤ì •</h3>
      <label>
        <input type="checkbox" />
        ìë™ ê²€ì‚¬ í™œì„±í™”
      </label>
    </div>
  )
};

async function checkGrammar(text: string): Promise<GrammarError[]> {
  // í•œê¸€ ë§ì¶¤ë²• API í˜¸ì¶œ
  const response = await fetch('https://api.example.com/grammar', {
    method: 'POST',
    body: JSON.stringify({ text })
  });

  return response.json();
}
```

---

## 6.2 Public API

```typescript
// src/api/public/index.ts

import express from 'express';
import cors from 'cors';

const app = express();
app.use(cors());
app.use(express.json());

// GET /api/memos - ì „ì²´ ë©”ëª¨ ì¡°íšŒ
app.get('/api/memos', async (req, res) => {
  const { page = 1, limit = 20, search } = req.query;

  const memos = await db.memos
    .where('content')
    .startsWithIgnoreCase(search as string || '')
    .offset((+page - 1) * +limit)
    .limit(+limit)
    .toArray();

  res.json({
    data: memos,
    pagination: {
      page: +page,
      limit: +limit,
      total: await db.memos.count()
    }
  });
});

// POST /api/memos - ìƒˆ ë©”ëª¨ ìƒì„±
app.post('/api/memos', async (req, res) => {
  const { title, content } = req.body;

  if (!title || !content) {
    return res.status(400).json({ error: 'ì œëª©ê³¼ ë‚´ìš©ì€ í•„ìˆ˜ì…ë‹ˆë‹¤' });
  }

  const memo = await createMemo({ title, content });

  res.status(201).json({ data: memo });
});

// GET /api/ideas?memoId=xxx - ë©”ëª¨ì˜ ì•„ì´ë””ì–´ ì¡°íšŒ
app.get('/api/ideas', async (req, res) => {
  const { memoId } = req.query;

  if (!memoId) {
    return res.status(400).json({ error: 'memoIdëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤' });
  }

  const ideas = await db.ideas
    .where('memoId')
    .equals(memoId as string)
    .toArray();

  res.json({ data: ideas });
});

// GET /api/graph?memoId=xxx - ê·¸ë˜í”„ ë°ì´í„°
app.get('/api/graph', async (req, res) => {
  const { memoId } = req.query;

  const nodes = await db.ideas.where('memoId').equals(memoId as string).toArray();
  const edges = await db.connections.where('memoId').equals(memoId as string).toArray();

  res.json({
    nodes,
    edges
  });
});

app.listen(3000, () => {
  console.log('ğŸš€ API server running on http://localhost:3000');
});
```

---

## 6.3 í†µí•© (Notion, Obsidian, Zapier)

### Notion ë‚´ë³´ë‚´ê¸°

```typescript
// src/integrations/notion.ts

import { Client } from '@notionhq/client';

export async function exportToNotion(memo: Memo, ideas: Idea[]) {
  const notion = new Client({ auth: process.env.NOTION_API_KEY });

  // í˜ì´ì§€ ìƒì„±
  const page = await notion.pages.create({
    parent: { database_id: process.env.NOTION_DATABASE_ID! },
    properties: {
      Name: {
        title: [{ text: { content: memo.title } }]
      },
      Created: {
        date: { start: memo.createdAt.toISOString() }
      }
    },
    children: [
      // ë©”ëª¨ ë‚´ìš©
      {
        type: 'paragraph',
        paragraph: {
          rich_text: [{ text: { content: memo.content } }]
        }
      },
      // ì—°ê²°ëœ ì•„ì´ë””ì–´ë“¤
      {
        type: 'heading_2',
        heading_2: {
          rich_text: [{ text: { content: 'ì—°ê²°ëœ ì•„ì´ë””ì–´' } }]
        }
      },
      ...ideas.map(idea => ({
        type: 'quote' as const,
        quote: {
          rich_text: [{
            text: {
              content: `"${idea.content}" - ${idea.source.author}`
            }
          }]
        }
      }))
    ]
  });

  return page;
}
```

### Obsidian ë§ˆí¬ë‹¤ìš´ ë³€í™˜

```typescript
// src/integrations/obsidian.ts

export function convertToObsidianMarkdown(memo: Memo, ideas: Idea[]): string {
  let markdown = `# ${memo.title}\n\n`;
  markdown += `Created: ${memo.createdAt.toLocaleDateString()}\n\n`;
  markdown += `${memo.content}\n\n`;
  markdown += `---\n\n`;
  markdown += `## ì—°ê²°ëœ ì•„ì´ë””ì–´\n\n`;

  for (const idea of ideas) {
    markdown += `### ${idea.source.author}\n\n`;
    markdown += `> ${idea.content}\n\n`;
    markdown += `Tags: ${idea.keywords?.join(', ')}\n\n`;
  }

  return markdown;
}

export function downloadAsMarkdown(memo: Memo, ideas: Idea[]) {
  const markdown = convertToObsidianMarkdown(memo, ideas);
  const blob = new Blob([markdown], { type: 'text/markdown' });
  const url = URL.createObjectURL(blob);

  const link = document.createElement('a');
  link.href = url;
  link.download = `${memo.title}.md`;
  link.click();

  URL.revokeObjectURL(url);
}
```

---

# Part 7: êµ¬í˜„ ë¡œë“œë§µ

## 7.1 16ì£¼ ë‹¨ê³„ë³„ ê³„íš

### Phase 1: Foundation (Week 1-4)
**ëª©í‘œ**: ê¸°ë³¸ ì¸í”„ë¼ + 10,000ê°œ ë…¸ë“œ

```bash
Week 1:
  âœ“ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ìµœì¢… í™•ì •
  âœ“ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
  âœ“ NLP ëª¨ë¸ ì…‹ì—… (spaCy, Sentence Transformers)

Week 2-3:
  âœ“ 10,000ê°œ ë…¸ë“œ ìˆ˜ì§‘ (Quotable, Wikiquote)
  âœ“ ê¸°ë³¸ ì„ë² ë”© ìƒì„±
  âœ“ FAISS ì¸ë±ìŠ¤ êµ¬ì¶•

Week 4:
  âœ“ ìë™ ê²€ì¦ ì‹œìŠ¤í…œ
  âœ“ NAS ì €ì¥ì†Œ ì„¤ì •
  âœ“ ì²« ë°°ì¹˜ ë°ì´í„° ê²€ì¦
```

### Phase 2: Enrichment (Week 5-8)
**ëª©í‘œ**: ì–¸ì–´í•™Â·ì‹¬ë¦¬í•™ì  ì£¼ì„ + 30,000ê°œ ë…¸ë“œ

```bash
Week 5-6:
  âœ“ í”„ë ˆì„ ì˜ë¯¸ë¡  ë¶„ì„ê¸°
  âœ“ ì€ìœ  ê°ì§€ ì‹œìŠ¤í…œ
  âœ“ ê°ì • ë¶„ì„ (Plutchik 8ê°ì •)

Week 7-8:
  âœ“ 30,000ê°œ ì¶”ê°€ ë…¸ë“œ ìˆ˜ì§‘
  âœ“ ì „ì²´ ë…¸ë“œ ê°•í™” (enrichment)
  âœ“ í’ˆì§ˆ ê²€ì¦ ë° íë ˆì´ì…˜
```

### Phase 3: Graph Construction (Week 9-12)
**ëª©í‘œ**: 50,000ê°œ ë…¸ë“œ + 500,000ê°œ ì—£ì§€

```bash
Week 9-10:
  âœ“ ê´€ê³„ ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
  âœ“ ì—£ì§€ ìë™ ìƒì„± (ë‹¤ì°¨ì› ìœ ì‚¬ë„)
  âœ“ ê·¸ë˜í”„ ìµœì í™”

Week 11-12:
  âœ“ ì»¤ë®¤ë‹ˆí‹° ê°ì§€ (Louvain)
  âœ“ ì¤‘ì‹¬ì„± ê³„ì‚° (PageRank)
  âœ“ 50,000ê°œ ë…¸ë“œ ì™„ì„±
```

### Phase 4: UI/UX (Week 13-14)
**ëª©í‘œ**: ì˜µì‹œë””ì–¸ ìŠ¤íƒ€ì¼ UI + í†µí•©

```bash
Week 13:
  âœ“ 6ê°€ì§€ í…Œë§ˆ êµ¬í˜„
  âœ“ ë§ˆì´í¬ë¡œ ì¸í„°ë™ì…˜
  âœ“ ì´ë¯¸ì§€ ê³µìœ  ê¸°ëŠ¥

Week 14:
  âœ“ 3D ê·¸ë˜í”„ ì‹œê°í™”
  âœ“ ë°˜ì‘í˜• ë””ìì¸
  âœ“ ì ‘ê·¼ì„± ê°œì„ 
```

### Phase 5: Quality & Testing (Week 15-16)
**ëª©í‘œ**: ì•ˆì •í™” + ë°°í¬

```bash
Week 15:
  âœ“ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (90% ì»¤ë²„ë¦¬ì§€)
  âœ“ í†µí•© í…ŒìŠ¤íŠ¸
  âœ“ ì„±ëŠ¥ ìµœì í™”

Week 16:
  âœ“ ì „ë¬¸ê°€ íë ˆì´ì…˜ (ìƒìœ„ 10%)
  âœ“ ìµœì¢… ê²€ì¦
  âœ“ NAS ë°°í¬
  âœ“ GitHub Pages ì—…ë°ì´íŠ¸
```

---

## 7.2 ë§ˆì¼ìŠ¤í†¤ & KPI

### Milestone 1 (Week 4): ê¸°ë°˜ ì™„ì„±
- [x] 10,000ê°œ ë…¸ë“œ ìˆ˜ì§‘
- [x] ì„ë² ë”© ì‹œìŠ¤í…œ êµ¬ì¶•
- [x] NAS ì €ì¥ì†Œ ì„¤ì •
- **KPI**: ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ > 80

### Milestone 2 (Week 8): ê°•í™” ì™„ë£Œ
- [ ] 40,000ê°œ ë…¸ë“œ (ëˆ„ì )
- [ ] ì–¸ì–´í•™Â·ì‹¬ë¦¬í•™ ì£¼ì„ 100%
- **KPI**: ë©”íƒ€ë°ì´í„° ì™„ì„±ë„ > 85%

### Milestone 3 (Week 12): ê·¸ë˜í”„ ì™„ì„±
- [ ] 52,000ê°œ ë…¸ë“œ
- [ ] 500,000ê°œ ì´ìƒ ì—£ì§€
- **KPI**: í‰ê·  ì—°ê²°ë„ > 10, ê²€ìƒ‰ ì •í™•ë„ > 85%

### Milestone 4 (Week 14): UI ì™„ì„±
- [ ] 6ê°€ì§€ í…Œë§ˆ
- [ ] 3D ê·¸ë˜í”„ ì‹œê°í™”
- **KPI**: ì‚¬ìš©ì ì²´ë¥˜ ì‹œê°„ > 10ë¶„

### Milestone 5 (Week 16): ëŸ°ì¹­
- [ ] í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ > 90%
- [ ] ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ
- **KPI**: Lighthouse ì ìˆ˜ > 90

---

## 7.3 ì„±ê³µ ì§€í‘œ (ì¢…í•©)

### ì •ëŸ‰ì  ì§€í‘œ

| ì§€í‘œ | ëª©í‘œ | ì¸¡ì • ë°©ë²• |
|------|------|----------|
| **ë…¸ë“œ ìˆ˜** | 52,000+ | master_index.json |
| **ì—£ì§€ ìˆ˜** | 500,000+ | ê·¸ë˜í”„ í†µê³„ |
| **ì—£ì§€ í’ˆì§ˆ** | í‰ê·  0.7+ | confidence ì ìˆ˜ |
| **ê²€ìƒ‰ ì •í™•ë„** | 85%+ | ì‚¬ìš©ì í”¼ë“œë°± |
| **ì‘ë‹µ ì‹œê°„** | < 200ms | Performance API |
| **ë²ˆë“¤ í¬ê¸°** | < 500KB | Vite build ë¶„ì„ |

### ì •ì„±ì  ì§€í‘œ

- **ë©”íƒ€ë°ì´í„° ì™„ì„±ë„**: 80% ì´ìƒ ë…¸ë“œê°€ ëª¨ë“  ì°¨ì› ì£¼ì„ ë³´ìœ 
- **ì»¤ë®¤ë‹ˆí‹° êµ¬ì¡°**: 10-20ê°œ ëª…í™•í•œ ì£¼ì œ í´ëŸ¬ìŠ¤í„°
- **ë‹¤ì–‘ì„±**: 10ê°œ ì¹´í…Œê³ ë¦¬ ê· í˜•ì¡íŒ ë¶„í¬

### ì‚¬ìš©ì ê²½í—˜ ì§€í‘œ

- **ë°œê²¬ ë¹„ìœ¨**: ì„¸ì…˜ë‹¹ 2ê°œ ì´ìƒ ì˜ˆìƒì¹˜ ëª»í•œ ìœ ìš©í•œ ì—°ê²° ë°œê²¬
- **ì €ì¥ë¥ **: íƒìƒ‰í•œ ë…¸ë“œì˜ 10% ì´ìƒ ì €ì¥
- **í†µí•©ë¥ **: ì‚¬ìš©ì ë©”ëª¨ì˜ 30% ì´ìƒì´ DB ë…¸ë“œì™€ ì—°ê²°

---

## 7.4 ëŸ°ì¹­ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì½”ë“œ í’ˆì§ˆ
- [ ] ESLint ê²½ê³  0ê°œ
- [ ] TypeScript ì—ëŸ¬ 0ê°œ
- [ ] í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ > 90%
- [ ] ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ë¬¸ì„œí™”

### ì„±ëŠ¥
- [ ] Lighthouse Performance > 90
- [ ] First Contentful Paint < 1.5s
- [ ] Time to Interactive < 3s
- [ ] ë²ˆë“¤ í¬ê¸° < 500KB

### ì ‘ê·¼ì„±
- [ ] WCAG 2.1 AA ì¤€ìˆ˜
- [ ] í‚¤ë³´ë“œ ë„¤ë¹„ê²Œì´ì…˜ 100%
- [ ] ìŠ¤í¬ë¦° ë¦¬ë” í…ŒìŠ¤íŠ¸ ì™„ë£Œ
- [ ] ìƒ‰ìƒ ëŒ€ë¹„ 4.5:1 ì´ìƒ

### ë°ì´í„°
- [ ] 52,000ê°œ ë…¸ë“œ ê²€ì¦ ì™„ë£Œ
- [ ] 500,000ê°œ ì—£ì§€ ê²€ì¦ ì™„ë£Œ
- [ ] NAS ë°±ì—… ì™„ë£Œ
- [ ] ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ êµ¬ì¶•

### ë°°í¬
- [ ] GitHub Pages ë¹Œë“œ ì„±ê³µ
- [ ] HTTPS ì„¤ì • ì™„ë£Œ
- [ ] ì»¤ìŠ¤í…€ ë„ë©”ì¸ ì—°ê²°
- [ ] CDN ì„¤ì • (ì„ íƒ)

### ë¬¸ì„œí™”
- [ ] README.md ì—…ë°ì´íŠ¸
- [ ] ì‚¬ìš©ì ê°€ì´ë“œ ì‘ì„±
- [ ] API ë¬¸ì„œ ì‘ì„±
- [ ] ê¸°ì—¬ ê°€ì´ë“œë¼ì¸

---

**ğŸ‰ IdeaConnect v2.0 ì™„ë²½ ê°€ì´ë“œ ì™„ì„±!**

ì´ ê°€ì´ë“œëŠ” ì´ë¡ ì  ê¹Šì´ì™€ ì‹¤ìš©ì  êµ¬í˜„, ê·¸ë¦¬ê³  UX í˜ì‹ ì„ ëª¨ë‘ ë‹´ì€ ì™„ì „í•œ ë¡œë“œë§µì…ë‹ˆë‹¤.

**ë‹¤ìŒ ë‹¨ê³„:**
1. Partë³„ë¡œ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ êµ¬í˜„
2. ê° Phase ì™„ë£Œ ì‹œ ë§ˆì¼ìŠ¤í†¤ ì²´í¬
3. ì§€ì†ì ì¸ ê°œì„  ë° ì»¤ë®¤ë‹ˆí‹° í”¼ë“œë°± ë°˜ì˜

**ë¬¸ì˜ ë° ê¸°ì—¬**: [GitHub Issues](https://github.com/yourusername/ideaconnect)

---

**ë²„ì „**: 2.0
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-11-10
**ì‘ì„±ì**: Claude + IdeaConnect Team

